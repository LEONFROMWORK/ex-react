#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ê²Œì‹œíŒ ê°•í™” í¬ë¡¤ëŸ¬ (v5) - ê³ í’ˆì§ˆ ë°ì´í„° ì „ìš©
- ê°•í™”ëœ ì‹œê°„ ì •ë³´ ì œê±°
- ê°œì„ ëœ ë‹µë³€ ê¸¸ì´ í•„í„°ë§ (50-500ì)
- ê°•í™”ëœ ë…¸ì´ì¦ˆ ì œê±°
- ì—„ê²©í•œ Excel í‚¤ì›Œë“œ í•„í„°ë§
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs
import re

class OppaduCrawlerV5Enhanced:
    def __init__(self):
        self.base_url = "https://www.oppadu.com/community/question/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Excel í‚¤ì›Œë“œ í™•ì¥
        self.excel_keywords = [
            # í•µì‹¬ í•¨ìˆ˜ë“¤
            'VLOOKUP', 'HLOOKUP', 'INDEX', 'MATCH', 'IF', 'SUMIF', 'COUNTIF',
            'SUMIFS', 'COUNTIFS', 'AVERAGEIF', 'AVERAGEIFS', 'IFERROR', 'ISERROR',
            'SUM', 'COUNT', 'AVERAGE', 'MAX', 'MIN', 'CONCATENATE', 'LEFT', 'RIGHT',
            'MID', 'LEN', 'FIND', 'SEARCH', 'SUBSTITUTE', 'REPLACE', 'TRIM',
            'UPPER', 'LOWER', 'PROPER', 'TODAY', 'NOW', 'DATE', 'TIME',
            'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND',
            
            # í•œê¸€ í‚¤ì›Œë“œë“¤
            'ì—‘ì…€', 'í•¨ìˆ˜', 'ìˆ˜ì‹', 'ì…€', 'ì‹œíŠ¸', 'ì›Œí¬ì‹œíŠ¸', 'í”¼ë²—í…Œì´ë¸”', 'í”¼ë²—',
            'ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ë§¤í¬ë¡œ', 'VBA', 'ì¡°ê±´ë¶€ì„œì‹', 'ë°ì´í„°ê²€ì¦',
            'í•„í„°', 'ì •ë ¬', 'ë²”ìœ„', 'ì°¸ì¡°', 'ì ˆëŒ€ì°¸ì¡°', 'ìƒëŒ€ì°¸ì¡°', 'í˜¼í•©ì°¸ì¡°',
            'ì„œì‹', 'ì¡°ê±´', 'ê³„ì‚°', 'ê³µì‹', 'ì…ë ¥', 'ì¶œë ¥', 'ì¸ì‡„', 'í˜ì´ì§€ì„¤ì •',
            
            # íŠ¹ìˆ˜ ê¸°í˜¸ë“¤
            '=', '$', '#REF!', '#VALUE!', '#NAME?', '#DIV/0!', '#N/A', '#NULL!',
            
            # í‚¤ë³´ë“œ ì¡°í•©
            'Ctrl+', 'Alt+', 'Shift+', 'F2', 'F4', 'F9', 'F11', 'F12'
        ]
        
    def get_page_list(self, page_num):
        """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = f"{self.base_url}?pg={page_num}"
            print(f"Fetching page {page_num}: {url}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            posts = []
            
            # ì‹¤ì œ ì˜¤ë¹ ë‘ íŒ¨í„´: ?board_id=1&action=view&uid=XXXXX&pg=1
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´ í™•ì¸
                if (href and 
                    'action=view' in href and 
                    'uid=' in href and 
                    'board_id=' in href and
                    len(text) > 10 and  # ì œëª©ì´ ìˆëŠ” ë§í¬
                    not any(skip in text.lower() for skip in ['home', 'menu', 'login', 'í™ˆ', 'ë©”ë‰´', 'ë¡œê·¸ì¸'])):
                    
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith('?'):
                        full_url = self.base_url + href
                    elif href.startswith('/'):
                        full_url = "https://www.oppadu.com" + href
                    else:
                        full_url = href
                    
                    posts.append({
                        'url': full_url,
                        'title': text
                    })
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_posts = []
            for post in posts:
                if post['url'] not in seen:
                    seen.add(post['url'])
                    unique_posts.append(post)
            
            return unique_posts
            
        except Exception as e:
            print(f"Error fetching page {page_num}: {e}")
            return []
    
    def enhanced_clean_text(self, text):
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ - ì‹œê°„ ì •ë³´ ì™„ì „ ì œê±°"""
        if not text:
            return ""
        
        # 1. HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        text = text.replace('&nbsp;', ' ').replace('&quot;', '"').replace('&#39;', "'")
        
        # 2. HTML íƒœê·¸ ì™„ì „ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # 3. ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\u2640-\u2642" 
            u"\u2600-\u2B55"
            u"\u200d"
            u"\u23cf"
            u"\u23e9"
            u"\u231a"
            u"\ufe0f"  # dingbats
            u"\u3030"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub('', text)
        
        # 4. ê°•í™”ëœ ì‹œê°„/ë‚ ì§œ ì •ë³´ ì œê±°
        time_patterns = [
            # ë‚ ì§œ íŒ¨í„´ë“¤
            r'\d{4}[.-/]\d{1,2}[.-/]\d{1,2}',     # YYYY-MM-DD, YYYY.MM.DD, YYYY/MM/DD
            r'\d{1,2}[.-/]\d{1,2}[.-/]\d{4}',     # MM-DD-YYYY, MM.DD.YYYY, MM/DD/YYYY
            r'\d{4}\s*\d{1,2}\s*\d{1,2}',         # YYYY MM DD (ê³µë°± êµ¬ë¶„)
            r'\d{1,2}\s*\d{1,2}\s*\d{4}',         # MM DD YYYY (ê³µë°± êµ¬ë¶„)
            
            # ì‹œê°„ íŒ¨í„´ë“¤
            r'\d{1,2}:\d{2}(:\d{2})?(\s*(AM|PM|ì˜¤ì „|ì˜¤í›„))?',  # HH:MM, HH:MM:SS, AM/PM
            r'\d{1,2}ì‹œ\s*\d{1,2}ë¶„(\s*\d{1,2}ì´ˆ)?',          # Nì‹œ Në¶„ Nì´ˆ
            
            # ìƒëŒ€ ì‹œê°„ íŒ¨í„´ë“¤
            r'\d+\s*(ì´ˆ|ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”|ë…„)\s*(ì „|í›„|ë’¤|ì§€ë‚˜ì„œ)',
            r'\d+\s*(second|minute|hour|day|week|month|year)s?\s*(ago|later)',
            
            # í…ìŠ¤íŠ¸ ë ìˆ«ì íŒ¨í„´ë“¤ (ì‹œê°„ ì •ë³´ë¡œ ì¶”ì •)
            r'\s+\d{1,2}\s+\d{1,2}$',             # ëì— "ìˆ«ì ìˆ«ì" (ì˜ˆ: " 6 2")
            r'\s+\d{1,2}$',                       # ëì— ë‹¨ì¼ ìˆ«ì (ì˜ˆ: " 12")
            r'\d{1,2}\s*\d{1,2}$',                # ëì— ë¶™ì–´ìˆëŠ” ìˆ«ìë“¤ (ì˜ˆ: "12 2")
            
            # ê¸°íƒ€ ì‹œê°„ ê´€ë ¨
            r'ì‘ì„±ì¼ì‹œ?[:ï¼š]\s*\d+',
            r'ë“±ë¡ì¼ì‹œ?[:ï¼š]\s*\d+',
            r'ìˆ˜ì •ì¼ì‹œ?[:ï¼š]\s*\d+',
            r'\d+\.\d+\.\d+\s*\d+:\d+',
            
            # Lv, ë ˆë²¨ ì •ë³´
            r'Lv\.\s*\d+',
            r'ë ˆë²¨\s*\d+',
            r'LV\s*\d+',
        ]
        
        for pattern in time_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
        
        # 5. ì˜¤ë¹ ë‘ íŠ¹í™” ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±° (í™•ì¥)
        noise_patterns = [
            r'ì˜¤ë¹ ë‘ì—‘ì…€.*?ì»¤ë®¤ë‹ˆí‹°',
            r'ì—‘ì…€ê°•ì˜.*?ëŒ€í‘œì±„ë„',
            r'ì¢‹ì•„ìš”\s*\d+',
            r'ëŒ“ê¸€\s*\d+',
            r'ì¡°íšŒ\s*\d+',
            r'ì¶”ì²œ\s*\d+',
            r'ìŠ¤í¬ë©\s*\d+',
            r'ì‹ ê³ ',
            r'ì±„íƒëœ\s*ë‹µë³€',
            r'ì±„íƒ\s*ì™„ë£Œ',
            r'ë² ìŠ¤íŠ¸\s*ë‹µë³€',
            r'@\w+ë‹˜?',
            r'@[ê°€-í£]+ë‹˜?',
            r'ì§ˆë¬¸ì\s*:?',
            r'ë‹µë³€ì\s*:?',
            r'ì‘ì„±ì\s*:?',
            r'ê¸€ì“´ì´\s*:?',
            r'https?://[^\s]+',          # URL
            r'www\.[^\s]+',              # www ë§í¬
            r'[ê°€-í£]+\.com[^\s]*',       # í•œê¸€ë„ë©”ì¸
            r'ì²¨ë¶€íŒŒì¼\s*:?',
            r'íŒŒì¼\s*ì²¨ë¶€',
            r'ì´ë¯¸ì§€\s*ì²¨ë¶€',
            r'ìŠ¤í¬ë¦°ìƒ·',
            r'ìº¡ì²˜',
            r'ê·¸ë¦¼\s*\d*',
            r'ì´ë¯¸ì§€\s*\d*',
            r'ì¶œì²˜\s*:?',
            r'ì°¸ê³ \s*:?',
            r'Source\s*:?',
            r'â€».*?â€»',                   # ì£¼ì„ í‘œì‹œ
            r'\[.*?\]',                  # ëŒ€ê´„í˜¸ ë‚´ìš©
            r'ã€.*?ã€‘',                   # íŠ¹ìˆ˜ ê´„í˜¸
            r'â˜….*?â˜…',                   # ë³„í‘œ ë‚´ìš©
            r'â–¶.*?â—€',                   # í™”ì‚´í‘œ ë‚´ìš©
        ]
        
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
        
        # 6. ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n', text)
        
        # 7. íŠ¹ìˆ˜ë¬¸ì ì—°ì† ì œê±°
        text = re.sub(r'[^\w\sê°€-í£=\(\)\[\]\{\}\+\-\*\/\.\,\?\!]{2,}', ' ', text)
        
        # 8. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def has_excel_relevance(self, text):
        """Excel ê´€ë ¨ì„±ì„ ë” ì—„ê²©í•˜ê²Œ í™•ì¸í•©ë‹ˆë‹¤."""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # Excel í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
        keyword_count = 0
        for keyword in self.excel_keywords:
            if keyword.lower() in text_lower:
                keyword_count += 1
        
        # ìµœì†Œ 2ê°œ ì´ìƒì˜ Excel í‚¤ì›Œë“œê°€ ìˆì–´ì•¼ í•¨
        if keyword_count < 2:
            return False
        
        # ë˜ëŠ” í•µì‹¬ Excel í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ í†µê³¼
        core_functions = ['=', 'VLOOKUP', 'IF', 'SUM', 'COUNT', 'INDEX', 'MATCH']
        if any(func in text for func in core_functions):
            return True
        
        # ë˜ëŠ” ì—‘ì…€ ê´€ë ¨ í•œê¸€ í‚¤ì›Œë“œê°€ ì¶©ë¶„íˆ ìˆìœ¼ë©´ í†µê³¼
        korean_excel_words = ['ì—‘ì…€', 'í•¨ìˆ˜', 'ìˆ˜ì‹', 'ì…€', 'ì‹œíŠ¸', 'ì›Œí¬ì‹œíŠ¸']
        korean_count = sum(1 for word in korean_excel_words if word in text)
        
        return korean_count >= 2
    
    def is_high_quality_answer(self, answer_text):
        """ê³ í’ˆì§ˆ ë‹µë³€ì¸ì§€ ë” ì—„ê²©í•˜ê²Œ í™•ì¸í•©ë‹ˆë‹¤."""
        if not answer_text:
            return False
        
        # 1. ê¸¸ì´ í™•ì¸ (50-500ìë¡œ ì œí•œ)
        if len(answer_text) < 50 or len(answer_text) > 500:
            return False
        
        # 2. Excel ê´€ë ¨ì„± í™•ì¸
        if not self.has_excel_relevance(answer_text):
            return False
        
        # 3. ì €í’ˆì§ˆ íŒ¨í„´ í™•ì¸
        low_quality_patterns = [
            r'^[\d\s\.\-=\+\*\/\(\)]+$',     # ìˆ«ìì™€ ê¸°í˜¸ë§Œ
            r'^[ê°€-í£]{1,10}$',              # ë„ˆë¬´ ì§§ì€ í•œê¸€ë§Œ
            r'^\w{1,15}$',                   # ë„ˆë¬´ ì§§ì€ ì˜ë¬¸ë§Œ
            r'^[\?\!\.\,\s]+$',              # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^(ê°ì‚¬|ê³ ë§ˆì›Œ|ì•Œê² ì–´|ë„¤|ì˜ˆ|ë§ì•„|ê·¸ë˜|ì•„ë‹ˆ|ëª¨ë¦„).*?$',  # ë„ˆë¬´ ê°„ë‹¨í•œ ë‹µë³€
            r'^(thanks?|yes|no|ok|good)\s*$',  # ì˜ë¬¸ ê°„ë‹¨ ë‹µë³€
            r'^\s*\?\s*$',                   # ë¬¼ìŒí‘œë§Œ
            r'^\s*\!\s*$',                   # ëŠë‚Œí‘œë§Œ
        ]
        
        for pattern in low_quality_patterns:
            if re.match(pattern, answer_text, re.IGNORECASE):
                return False
        
        # 4. ë„ì›€ë˜ëŠ” í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        helpful_patterns = [
            r'[ê°€-í£]+í•¨ìˆ˜',                 # ~í•¨ìˆ˜
            r'=\w+\(',                      # í•¨ìˆ˜ í˜¸ì¶œ
            r'ì…€\s*[A-Z]\d+',                # ì…€ ì°¸ì¡°
            r'ì‹œíŠ¸\d*',                     # ì‹œíŠ¸ ì–¸ê¸‰
            r'ë°©ë²•',                        # ë°©ë²• ì„¤ëª…
            r'ì–´ë–»ê²Œ',                      # ì„¤ëª…
            r'ë‹¨ê³„',                        # ë‹¨ê³„ë³„ ì„¤ëª…
        ]
        
        helpful_count = sum(1 for pattern in helpful_patterns 
                          if re.search(pattern, answer_text, re.IGNORECASE))
        
        return helpful_count >= 1
    
    def get_post_content(self, post_url, post_title):
        """ê²Œì‹œê¸€ì—ì„œ ì§ˆë¬¸ê³¼ ì±„íƒëœ ë‹µë³€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            print(f"  Processing: {post_title[:50]}...")
            response = self.session.get(post_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 1. ì§ˆë¬¸ ì œëª© ì¶”ì¶œ
            question_title = ""
            title_selectors = [
                'h1', 'h2', 'h3',
                '.post-title',
                '.question-title',
                '.board-title',
                'title'
            ]
            
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem and elem.get_text(strip=True):
                    question_title = self.enhanced_clean_text(elem.get_text())
                    break
            
            if not question_title:
                question_title = self.enhanced_clean_text(post_title)
            
            # 2. ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„)
            question_content = ""
            content_selectors = [
                '.post-content',
                '.question-content',
                '.view-content', 
                '.board-content',
                'article',
                '.content',
                '.text-content'
            ]
            
            for selector in content_selectors:
                elem = soup.select_one(selector)
                if elem:
                    question_content = self.enhanced_clean_text(elem.get_text())
                    if len(question_content) > 20:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´
                        break
            
            # 3. ì±„íƒëœ ë‹µë³€ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
            selected_answers = []
            
            answer_selectors = [
                '.comment-wrapper.selected-answer',
                '.selected-answer',
                '.best-answer',
                '.accepted-answer',
                '.answer.selected',
                '.reply.selected',
                '[class*="selected"]',
                '[class*="best"]',
                '[class*="accepted"]'
            ]
            
            for selector in answer_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    answer_text = self.enhanced_clean_text(elem.get_text())
                    
                    # ê³ í’ˆì§ˆ ê²€ì¦
                    if self.is_high_quality_answer(answer_text):
                        selected_answers.append(answer_text)
                
                if selected_answers:
                    break
            
            # ì±„íƒëœ ë‹µë³€ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë‹µë³€ì—ì„œ ì°¾ê¸°
            if not selected_answers:
                general_answer_selectors = [
                    '.comment-content',
                    '.reply-content',
                    '.answer-content',
                    '.comment',
                    '.reply'
                ]
                
                for selector in general_answer_selectors:
                    elements = soup.select(selector)
                    for elem in elements:
                        answer_text = self.enhanced_clean_text(elem.get_text())
                        
                        # ë” ì—„ê²©í•œ ê³ í’ˆì§ˆ ê²€ì¦
                        if self.is_high_quality_answer(answer_text):
                            selected_answers.append(answer_text)
                            break  # ì²« ë²ˆì§¸ ì¢‹ì€ ë‹µë³€ë§Œ
                    
                    if selected_answers:
                        break
            
            # ë‹µë³€ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not selected_answers:
                print(f"    âœ— No high-quality answers found")
                return None
            
            # 5. ìµœì¢… ê²€ì¦
            final_question = self.build_question(question_title, question_content)
            final_answer = selected_answers[0]  # ì²« ë²ˆì§¸ ë‹µë³€ë§Œ
            
            # ì§ˆë¬¸ë„ Excel ê´€ë ¨ì„± í™•ì¸
            if (len(final_question) < 20 or 
                not self.has_excel_relevance(final_question)):
                print(f"    âœ— Question quality/relevance check failed")
                return None
            
            return {
                'question': final_question,
                'answer': final_answer
            }
            
        except Exception as e:
            print(f"    âœ— Error: {e}")
            return None
    
    def build_question(self, title, content):
        """ì§ˆë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        parts = []
        
        if title and len(title) > 5:
            parts.append(title)
        
        if content and len(content) > 10 and content != title:
            parts.append(content)
        
        return '\n\n'.join(parts) if parts else title or ""
    
    def save_data(self, data, filename):
        """ë°ì´í„°ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                for item in data:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
            print(f"Saved {len(data)} items to {filename}")
        except Exception as e:
            print(f"Error saving data: {e}")
    
    def crawl_pages(self, start_page=1, end_page=300, output_file='../data/oppadu_enhanced_qa.jsonl'):
        """ê°•í™”ëœ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_data = []
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
        
        print(f"=== ì˜¤ë¹ ë‘ ê°•í™” í¬ë¡¤ë§ (v5): {start_page}-{end_page} í˜ì´ì§€ ===")
        print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"ë‹µë³€ ê¸¸ì´ ì œí•œ: 50-500ì")
        print(f"Excel í‚¤ì›Œë“œ í•„ìˆ˜: {len(self.excel_keywords)}ê°œ í‚¤ì›Œë“œ pool")
        
        for page_num in range(start_page, end_page + 1):
            print(f"\n--- Page {page_num}/{end_page} ---")
            
            posts = self.get_page_list(page_num)
            
            if not posts:
                print(f"No posts found on page {page_num}")
                continue
            
            print(f"Found {len(posts)} posts")
            
            page_success = 0
            for i, post in enumerate(posts):
                qa_data = self.get_post_content(post['url'], post['title'])
                
                if qa_data:
                    all_data.append(qa_data)
                    page_success += 1
                    print(f"    âœ“ Success ({page_success}): {qa_data['question'][:40]}...")
                    
                    # 5ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                    if len(all_data) % 5 == 0:
                        self.save_data(all_data, output_file)
                
                # Rate limiting
                time.sleep(2.5)
            
            print(f"Page {page_num}: {page_success}/{len(posts)} collected (ì´ {len(all_data)}ê°œ)")
            
            # í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
            if all_data:
                self.save_data(all_data, output_file)
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            time.sleep(3)
            
            # 5í˜ì´ì§€ë§ˆë‹¤ í†µê³„
            if page_num % 5 == 0 and all_data:
                print(f"\n=== ì§„í–‰í†µê³„ (í˜ì´ì§€ {page_num}) ===")
                print(f"ì´ ìˆ˜ì§‘: {len(all_data)}ê°œ")
                print(f"í‰ê· : {len(all_data)/page_num:.1f}ê°œ/í˜ì´ì§€")
                
                avg_q_len = sum(len(item['question']) for item in all_data) / len(all_data)
                avg_a_len = sum(len(item['answer']) for item in all_data) / len(all_data)
                print(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.0f}ì")
                print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.0f}ì")
                
                # Excel í‚¤ì›Œë“œ í†µê³„
                excel_questions = sum(1 for item in all_data if self.has_excel_relevance(item['question']))
                excel_answers = sum(1 for item in all_data if self.has_excel_relevance(item['answer']))
                print(f"Excel ê´€ë ¨ ì§ˆë¬¸: {excel_questions}/{len(all_data)} ({excel_questions/len(all_data)*100:.1f}%)")
                print(f"Excel ê´€ë ¨ ë‹µë³€: {excel_answers}/{len(all_data)} ({excel_answers/len(all_data)*100:.1f}%)")
        
        # ìµœì¢… ì €ì¥
        if all_data:
            self.save_data(all_data, output_file)
            
            # ìµœì¢… í†µê³„
            print(f"\nğŸ“Š ìµœì¢… í’ˆì§ˆ í†µê³„:")
            avg_q_len = sum(len(item['question']) for item in all_data) / len(all_data)
            avg_a_len = sum(len(item['answer']) for item in all_data) / len(all_data)
            print(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.0f}ì")
            print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.0f}ì")
            
            short_answers = sum(1 for item in all_data if len(item['answer']) < 100)
            long_answers = sum(1 for item in all_data if len(item['answer']) > 300)
            print(f"ì§§ì€ ë‹µë³€ (<100ì): {short_answers}/{len(all_data)} ({short_answers/len(all_data)*100:.1f}%)")
            print(f"ê¸´ ë‹µë³€ (>300ì): {long_answers}/{len(all_data)} ({long_answers/len(all_data)*100:.1f}%)")
            
        print(f"\nğŸ‰ ê°•í™” í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì´ ìˆ˜ì§‘: {len(all_data)}ê°œ ê³ í’ˆì§ˆ Q&A")
        print(f"íŒŒì¼: {output_file}")
        
        return len(all_data)

def main():
    crawler = OppaduCrawlerV5Enhanced()
    total_collected = crawler.crawl_pages(1, 300)
    print(f"\nìµœì¢… ê²°ê³¼: {total_collected}ê°œ ê°•í™”ëœ ê³ í’ˆì§ˆ Q&A ìˆ˜ì§‘ ì™„ë£Œ")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ì›¹ì‚¬ì´íŠ¸ HTML êµ¬ì¡° ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import re

def analyze_oppadu_structure():
    """ì˜¤ë¹ ë‘ ì›¹ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    url = "https://www.oppadu.com/community/question/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
    }
    
    session = requests.Session()
    session.headers.update(headers)
    
    print("=== ì˜¤ë¹ ë‘ HTML êµ¬ì¡° ë¶„ì„ ===")
    print(f"ë¶„ì„ URL: {url}")
    
    try:
        response = session.get(url, timeout=15)
        print(f"ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print("í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. ëª¨ë“  ë§í¬ ë¶„ì„
        print(f"\n1. ëª¨ë“  ë§í¬ ë¶„ì„:")
        all_links = soup.find_all('a', href=True)
        print(f"ì´ ë§í¬ ê°œìˆ˜: {len(all_links)}")
        
        # íŒ¨í„´ë³„ ë¶„ë¥˜
        patterns = {
            'view.php': [],
            'question': [],
            'no=': [],
            'idx=': [],
            'id=': [],
            'post': [],
            'board': [],
            'topic': [],
            'thread': [],
        }
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            for pattern, links_list in patterns.items():
                if pattern in href.lower():
                    links_list.append({
                        'href': href,
                        'text': text[:50] + '...' if len(text) > 50 else text
                    })
        
        # íŒ¨í„´ë³„ ê²°ê³¼ ì¶œë ¥
        for pattern, links_list in patterns.items():
            if links_list:
                print(f"\n'{pattern}' íŒ¨í„´ ë§í¬: {len(links_list)}ê°œ")
                for i, link in enumerate(links_list[:3]):
                    print(f"  {i+1}. {link['text']} -> {link['href']}")
        
        # 2. ê²Œì‹œê¸€ë¡œ ë³´ì´ëŠ” ë§í¬ ì°¾ê¸°
        print(f"\n2. ê²Œì‹œê¸€ í›„ë³´ ë§í¬:")
        post_candidates = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # ê²Œì‹œê¸€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì¡°ê±´ë“¤
            if (len(text) > 10 and  # ì œëª©ì´ ì–´ëŠ ì •ë„ ê¸¸ì´ê°€ ìˆìŒ
                not any(skip in text.lower() for skip in ['home', 'menu', 'login', 'register', 'search', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ê²€ìƒ‰', 'í™ˆ', 'ë©”ë‰´']) and  # ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œê°€ ì•„ë‹˜
                not href.startswith('#') and  # ì•µì»¤ ë§í¬ê°€ ì•„ë‹˜
                href not in ['/', '/community/', '/community/question/'] and  # ë©”ì¸ í˜ì´ì§€ê°€ ì•„ë‹˜
                '?' in href or 'view' in href.lower() or 'post' in href.lower() or 'topic' in href.lower()):  # ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´
                
                post_candidates.append({
                    'href': href,
                    'text': text
                })
        
        print(f"ê²Œì‹œê¸€ í›„ë³´: {len(post_candidates)}ê°œ")
        for i, candidate in enumerate(post_candidates[:10]):
            print(f"  {i+1}. {candidate['text'][:60]}...")
            print(f"      URL: {candidate['href']}")
        
        # 3. íŠ¹ì • CSS í´ë˜ìŠ¤ë‚˜ êµ¬ì¡° ì°¾ê¸°
        print(f"\n3. CSS êµ¬ì¡° ë¶„ì„:")
        
        # ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì°¾ê¸°
        lists = soup.find_all(['ul', 'ol', 'div'], class_=True)
        for i, element in enumerate(lists[:10]):
            class_name = element.get('class', [])
            children_with_links = element.find_all('a', href=True)
            if len(children_with_links) > 2:
                print(f"  ë¦¬ìŠ¤íŠ¸ {i+1}: class='{' '.join(class_name)}', ë§í¬ {len(children_with_links)}ê°œ")
        
        # í…Œì´ë¸” êµ¬ì¡° ì°¾ê¸°
        tables = soup.find_all('table')
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            links_in_table = table.find_all('a', href=True)
            if len(links_in_table) > 2:
                print(f"  í…Œì´ë¸” {i+1}: {len(rows)}í–‰, ë§í¬ {len(links_in_table)}ê°œ")
        
        # 4. í˜ì´ì§€ë„¤ì´ì…˜ ì°¾ê¸°
        print(f"\n4. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„:")
        pagination_keywords = ['page', 'pg', 'next', 'prev', 'ë‹¤ìŒ', 'ì´ì „', 'í˜ì´ì§€']
        pagination_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if any(keyword in href.lower() or keyword in text.lower() for keyword in pagination_keywords):
                pagination_links.append({
                    'href': href,
                    'text': text
                })
        
        print(f"í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬: {len(pagination_links)}ê°œ")
        for link in pagination_links[:5]:
            print(f"  {link['text']} -> {link['href']}")
        
        # 5. HTML ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"\n5. HTML ìƒ˜í”Œ (ì²˜ìŒ 2000ì):")
        html_content = str(soup)[:2000]
        print(html_content)
        
        return post_candidates, pagination_links
        
    except Exception as e:
        print(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        return [], []

if __name__ == "__main__":
    posts, pagination = analyze_oppadu_structure()
    
    if posts:
        print(f"\nğŸ‰ ê²Œì‹œê¸€ í›„ë³´ {len(posts)}ê°œ ë°œê²¬!")
        print(f"ì²« ë²ˆì§¸ ê²Œì‹œê¸€ URL íŒ¨í„´: {posts[0]['href']}")
    else:
        print(f"\nâŒ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ì´íŠ¸ê°€ JavaScriptë¡œ ë™ì  ë¡œë”©í•˜ê±°ë‚˜ ë‹¤ë¥¸ êµ¬ì¡°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.") 
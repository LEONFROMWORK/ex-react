#!/usr/bin/env python3
"""
ë°ì´í„° ì •ë¦¬ ë° ì¬ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
- ê¸°ì¡´ ì €í’ˆì§ˆ ë°ì´í„° ì™„ì „ ì‚­ì œ
- ìƒˆë¡œìš´ ê³ í’ˆì§ˆ í¬ë¡¤ë§ ì¤€ë¹„
"""

import os
import json
from datetime import datetime



def clean_data_directory():
    """ì˜ëª» ìˆ˜ì§‘ëœ ì €í’ˆì§ˆ ë°ì´í„°ë¥¼ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤."""
    data_dir = '../data'
    
    # ì‚­ì œí•  ì˜¤ë¹ ë‘ ì €í’ˆì§ˆ íŒŒì¼ë“¤ (Reddit ë°ì´í„°ëŠ” ìœ ì§€)
    files_to_remove = [
        'oppadu_qa_data.jsonl',           # ì›ë³¸ ì €í’ˆì§ˆ ì˜¤ë¹ ë‘ ë°ì´í„°
        'oppadu_qa_data_simple.jsonl',    # ë³€í™˜ëœ ì €í’ˆì§ˆ ì˜¤ë¹ ë‘ ë°ì´í„°
        'oppadu_simple_qa.jsonl',         # ë³€í™˜ëœ ì €í’ˆì§ˆ ì˜¤ë¹ ë‘ ë°ì´í„°
    ]
    
    removed_count = 0
    for filename in files_to_remove:
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"âœ— ì‚­ì œë¨: {filename}")
            removed_count += 1
    
    print(f"\nì´ {removed_count}ê°œ ì €í’ˆì§ˆ íŒŒì¼ ì™„ì „ ì‚­ì œ ì™„ë£Œ")

def wait_for_new_data():
    """ìƒˆë¡œìš´ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤."""
    data_dir = '../data'
    new_file = os.path.join(data_dir, 'oppadu_precise_qa.jsonl')
    
    print(f"\nìƒˆë¡œìš´ ê³ í’ˆì§ˆ ë°ì´í„° ëŒ€ê¸° ì¤‘...")
    print(f"ëŒ€ìƒ íŒŒì¼: {new_file}")
    
    if os.path.exists(new_file):
        with open(new_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"âœ“ ìƒˆë¡œìš´ ë°ì´í„° ë°œê²¬: {len(lines)}ê°œ Q&A")
            
            # ìƒ˜í”Œ ì¶œë ¥
            if lines:
                sample = json.loads(lines[0])
                print(f"\nìƒ˜í”Œ í’ˆì§ˆ í™•ì¸:")
                print(f"Question: {sample['question'][:100]}...")
                print(f"Answer: {sample['answer'][:100]}...")
                return True
    else:
        print("ì•„ì§ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("oppadu_crawler_v2.pyê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return False

def validate_new_data():
    """ìƒˆë¡œìš´ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    data_dir = '../data'
    new_file = os.path.join(data_dir, 'oppadu_precise_qa.jsonl')
    
    if not os.path.exists(new_file):
        print("ìƒˆë¡œìš´ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        with open(new_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if len(lines) == 0:
            print("ë°ì´í„° íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
        # ìƒ˜í”Œ ê²€ì¦
        valid_count = 0
        for i, line in enumerate(lines[:10]):  # ì²˜ìŒ 10ê°œ ìƒ˜í”Œ ê²€ì¦
            try:
                data = json.loads(line)
                if ('question' in data and 'answer' in data and 
                    len(data['question']) > 20 and len(data['answer']) > 20):
                    valid_count += 1
            except:
                pass
        
        quality_ratio = valid_count / min(10, len(lines))
        
        print(f"\n=== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ===")
        print(f"ì´ ë¼ì¸ ìˆ˜: {len(lines)}")
        print(f"ìƒ˜í”Œ ê²€ì¦: {valid_count}/10")
        print(f"í’ˆì§ˆ ë¹„ìœ¨: {quality_ratio*100:.1f}%")
        
        if quality_ratio >= 0.8:
            print("âœ“ ê³ í’ˆì§ˆ ë°ì´í„° í™•ì¸ë¨")
            return True
        else:
            print("âœ— ë°ì´í„° í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤")
            return False
            
    except Exception as e:
        print(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def create_final_dataset():
    """ìµœì¢… AI í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    data_dir = '../data'
    
    # ëª¨ë“  ê³ í’ˆì§ˆ ë°ì´í„° ìˆ˜ì§‘
    sources = [
        ('oppadu_precise_qa.jsonl', 'oppadu_v2'),
        ('reddit_qa_data.jsonl', 'reddit'),
        ('reddit_qa_data_part2.jsonl', 'reddit')
    ]
    
    final_data = []
    
    for filename, source in sources:
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            print(f"Processing {filename}...")
            
            with open(filepath, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        
                        # ì˜¤ë¹ ë‘ v2 ë°ì´í„°ëŠ” ì´ë¯¸ ì •ë¦¬ë¨
                        if source == 'oppadu_v2':
                            if 'question' in data and 'answer' in data:
                                final_data.append({
                                    'question': data['question'],
                                    'answer': data['answer']
                                })
                        
                        # Reddit ë°ì´í„°ëŠ” ê¸°ì¡´ í˜•ì‹ì—ì„œ ë³€í™˜
                        else:
                            question_parts = []
                            if 'title' in data and data['title']:
                                question_parts.append(data['title'])
                            if 'question' in data and data['question']:
                                question_parts.append(data['question'])
                            
                            question = '\n'.join(question_parts)
                            
                            answer = ""
                            if 'answers' in data and data['answers']:
                                if isinstance(data['answers'], list):
                                    answer = '\n\n'.join(data['answers'])
                                else:
                                    answer = str(data['answers'])
                            
                            if len(question) > 20 and len(answer) > 20:
                                final_data.append({
                                    'question': question,
                                    'answer': answer
                                })
                        
                    except Exception as e:
                        print(f"  Error processing line {line_num}: {e}")
            
            print(f"  Added {len([d for d in final_data if d]) - len(final_data)} items from {filename}")
    
    # ìµœì¢… íŒŒì¼ ì €ì¥
    final_file = os.path.join(data_dir, 'final_ai_training_data.jsonl')
    with open(final_file, 'w', encoding='utf-8') as f:
        for item in final_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"\n=== ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ ===")
    print(f"íŒŒì¼: {final_file}")
    print(f"ì´ Q&A ìŒ: {len(final_data)}")
    
    # í’ˆì§ˆ í†µê³„
    if final_data:
        avg_q_len = sum(len(item['question']) for item in final_data) / len(final_data)
        avg_a_len = sum(len(item['answer']) for item in final_data) / len(final_data)
        print(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.0f}ì")
        print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.0f}ì")
    
    return final_file

def main():
    print("=== ì˜¤ë¹ ë‘ ë°ì´í„° ì •ë¦¬ ë° ì¬ì‹œì‘ ===\n")
    
    # 1. ì €í’ˆì§ˆ ë°ì´í„° ì™„ì „ ì‚­ì œ
    print("1. ì €í’ˆì§ˆ ë°ì´í„° ì™„ì „ ì‚­ì œ...")
    clean_data_directory()
    
    # 2. ìƒˆë¡œìš´ ë°ì´í„° í™•ì¸
    print("\n2. ìƒˆë¡œìš´ í¬ë¡¤ë§ ë°ì´í„° í™•ì¸...")
    if wait_for_new_data():
        # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        print("\n3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦...")
        if validate_new_data():
            # 4. ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±
            print("\n4. ìµœì¢… AI í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±...")
            final_file = create_final_dataset()
            
            print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
            print(f"ìµœì¢… ë°ì´í„°: {final_file}")
        else:
            print("\nâŒ ë°ì´í„° í’ˆì§ˆì´ ê¸°ì¤€ì— ë¯¸ë‹¬í•©ë‹ˆë‹¤.")
    else:
        print("\nâ³ ìƒˆë¡œìš´ í¬ë¡¤ë§ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 
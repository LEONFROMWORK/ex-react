#!/usr/bin/env python3
"""
Oppadu ê²Œì‹œíŒ í¬ë¡¤ëŸ¬ (AI í•™ìŠµìš©)
https://www.oppadu.com/community/question/ ì—ì„œ Excel ê´€ë ¨ Q&A ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
AI í•™ìŠµì— ìµœì í™”ëœ ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
from urllib.parse import urljoin
import re

class OppaduCrawler:
    def __init__(self):
        self.base_url = "https://www.oppadu.com/community/question/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def get_page_list(self, page_num):
        """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = f"{self.base_url}?board_id=&pg={page_num}"
            print(f"Fetching page {page_num}: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            posts = []
            
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                
                if 'action=view' in href and 'uid=' in href:
                    if href.startswith('/'):
                        full_url = f"https://www.oppadu.com{href}"
                    elif href.startswith('?'):
                        full_url = f"{self.base_url}{href}"
                    else:
                        full_url = urljoin(self.base_url, href)
                    
                    title = link.get_text(strip=True)
                    if title and len(title) > 5:
                        posts.append({
                            'url': full_url,
                            'title': title
                        })
            
            seen = set()
            unique_posts = []
            for post in posts:
                if post['url'] not in seen:
                    seen.add(post['url'])
                    unique_posts.append(post)
            
            print(f"Found {len(unique_posts)} unique posts on page {page_num}")
            return unique_posts
            
        except Exception as e:
            print(f"Error fetching page {page_num}: {e}")
            return []
    
    def get_post_content(self, post_url, post_title):
        """ê°œë³„ ê²Œì‹œê¸€ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            print(f"  Fetching post: {post_title[:50]}...")
            response = self.session.get(post_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ
            content = ""
            
            # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë‚´ìš© ì¶”ì¶œ ì‹œë„
            for selector in [
                'article',
                'div[class*="content"]',
                'div[class*="post"]',
                'div[class*="view"]',
                'main',
                'td[class*="content"]',
                'td[class*="view"]',
                '#content',
                '.board-content',
                '.view-content'
            ]:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    break
            
            # ë‹µë³€/ëŒ“ê¸€ ì¶”ì¶œ
            answers = []
            for answer_div in soup.find_all(['div', 'section', 'td'], class_=re.compile('answer|reply|comment|response')):
                answer_text = answer_div.get_text(strip=True)
                if answer_text and len(answer_text) > 20:
                    answers.append(answer_text[:1000])
            
            return {
                'content': content[:2000] if content else post_title,
                'answers': answers[:3]
            }
            
        except Exception as e:
            print(f"  Error fetching post content: {e}")
            return None
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì„ ì œê±°í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        
        # ì´ëª¨ì§€ ë° ì´ëª¨í‹°ì½˜ ì œê±° (Unicode ë²”ìœ„)
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\u2640-\u2642" 
            u"\u2600-\u2B55"
            u"\u200d"
            u"\u23cf"
            u"\u23e9"
            u"\u231a"
            u"\ufe0f"  # dingbats
            u"\u3030"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub('', text)
        
        # ì¶”ê°€ íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°
        special_chars = ['ğŸ ', 'ğŸ“¢', 'ğŸ–ï¸', 'ğŸ’¬', 'ğŸš¨', 'ğŸ’¾', 'ğŸ¯', 'ğŸ“š', 'ğŸŸ¢', 'ğŸ“…', 'ğŸ‘', 'ğŸ“', 'ğŸ“„']
        for char in special_chars:
            text = text.replace(char, '')
        
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        patterns_to_remove = [
            r'ì˜¤ë¹ ë‘ì—‘ì…€ ì»¤ë®¤ë‹ˆí‹°.*?ì§„ì§œì“°ëŠ”ì‹¤ë¬´ì—‘ì…€',
            r'í˜„ì¬ ì ‘ì†ì.*?Agency',
            r'\d{4}ë…„ \d{2}ì›” \d{2}ì¼.*?ì¡°íšŒ \d+',
            r'ì—‘ì…€ë²„ì „.*?OSë²„ì „.*?ìœˆë„ìš°\d+',
            r'ì¢‹ì•„ìš”\d+ëŒ“ê¸€\d+ìŠ¤í¬ë©ê³µìœ ',
            r'ëŒ“ê¸€ì„ ì‘ì„±í•˜ë ¤ë©´ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.*?',
            r'ëª©ë¡â–² TOP.*',
            r'íŒŒì¼ ì²¨ë¶€ì €ì¥ì·¨ì†Œ\d+ì‹œê°„ ì „',
            r'ë¹„ì£¼ì–¼í…ìŠ¤íŠ¸.*?íŒŒì¼ ì²¨ë¶€ì €ì¥ì·¨ì†Œ',
            r'<img src="data:image.*?>',
            r'<pre lang=.*?</pre>',
            r'Lv\.\d+',
            r'@.*?ë‹˜',
            r'ì²¨ë¶€íŒŒì¼.*?KB\)',
            r'https?://[^\s]+',  # URL ì œê±°
            r'ê²Œì‹œê¸€ ëª©ë¡í˜ì´ì§€.*',
            r'â–² TOPê²Œì‹œê¸€.*',
            r'ë‹µë³€ ì™„ë£Œ.*?í•´ê²°.*?',
            r'ë‹µê¸€ \d+.*?ì¡°íšŒ\d+.*?',
            r'ì»¤ë®¤ë‹ˆí‹° ì „ì²´.*?',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def crawl_pages(self, start_page=1, end_page=50, output_file='oppadu_simple_qa.jsonl'):
        """ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        all_data = []
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
        
        for page_num in range(start_page, end_page + 1):
            print(f"\n--- Processing page {page_num}/{end_page} ---")
            
            posts = self.get_page_list(page_num)
            
            if not posts:
                print(f"No posts found on page {page_num}")
                continue
            
            print(f"Found {len(posts)} posts on page {page_num}")
            
            for i, post in enumerate(posts):
                post_data = self.get_post_content(post['url'], post['title'])
                
                if post_data:
                    # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ìƒì„± (ì œëª© + ë‚´ìš©)
                    question = f"{post['title']}\n{post_data['content']}"
                    question = self.clean_text(question)
                    
                    # ë‹µë³€ í…ìŠ¤íŠ¸ ìƒì„± (ëª¨ë“  ë‹µë³€ì„ í•˜ë‚˜ë¡œ í•©ì¹¨)
                    if post_data['answers']:
                        answer = '\n\n'.join(post_data['answers'])
                        answer = self.clean_text(answer)
                    else:
                        answer = ""
                    
                    # ìœ íš¨í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                    if len(question) > 50 and len(answer) > 20:
                        qa_data = {
                            'question': question,
                            'answer': answer
                        }
                        
                        all_data.append(qa_data)
                        print(f"  âœ“ Collected: {post['title'][:50]}...")
                        
                        # ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
                        if len(all_data) % 10 == 0:
                            self.save_data(all_data, output_file)
                    else:
                        print(f"  âœ— Skipped (insufficient content): {post['title'][:50]}...")
                
                time.sleep(1)
            
            time.sleep(2)
            
            self.save_data(all_data, output_file)
            print(f"Total collected: {len(all_data)} Q&A pairs")
        
        print(f"\nCrawling completed! Total: {len(all_data)} Q&A pairs")
        return all_data
    
    def save_data(self, data, output_file):
        """ë°ì´í„°ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"Data saved to {output_file} ({len(data)} items)")

def main():
    crawler = OppaduCrawler()
    
    # ë°ì´í„° ìˆ˜ì§‘ (ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ)
    output_path = '../data/oppadu_simple_qa.jsonl'
    crawler.crawl_pages(start_page=1, end_page=10, output_file=output_path)
    
    # ìˆ˜ì§‘ í†µê³„ ì¶œë ¥
    if os.path.exists(output_path):
        with open(output_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"\nTotal Q&A collected: {len(lines)}")
            
            # ìƒ˜í”Œ ì¶œë ¥
            if lines:
                sample = json.loads(lines[0])
                print("\nSample data:")
                print(f"Question: {sample['question'][:100]}...")
                print(f"Answer: {sample['answer'][:100]}...")

if __name__ == "__main__":
    main()
#!/usr/bin/env node

/**
 * AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
 * ìƒˆë¡œìš´ AI ì‹œìŠ¤í…œì„ ì„¤ì •í•˜ê³  ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 * 
 * ì‚¬ìš©ë²•: npx ts-node scripts/initialize-ai-system.ts
 */

import { prisma } from '@/lib/prisma';
import { initializeAISystem } from '@/lib/ai';
import { AIModelManager } from '@/lib/ai/model-manager';

async function main() {
  console.log('ğŸš€ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...\n');

  try {
    // 1. ê¸°ë³¸ AI ëª¨ë¸ ì„¤ì • ìƒì„±
    console.log('ğŸ“‹ ê¸°ë³¸ AI ëª¨ë¸ ì„¤ì • ìƒì„± ì¤‘...');
    
    const defaultModels = [
      {
        provider: 'openrouter',
        modelName: 'deepseek/deepseek-chat',
        displayName: 'DeepSeek Chat (ê²½ì œì )',
        description: 'ë¹„ìš© íš¨ìœ¨ì ì¸ ê¸°ë³¸ ì±„íŒ… ëª¨ë¸',
        isActive: true,
        isDefault: true,
        priority: 1,
        taskTypes: ['chat', 'general'],
        maxTokens: 2000,
        temperature: 0.7,
        costPerCredit: 0.0001,
        tier: 'TIER1'
      },
      {
        provider: 'openrouter',
        modelName: 'google/gemini-1.5-flash',
        displayName: 'Gemini 1.5 Flash',
        description: 'ë¹ ë¥¸ ë©€í‹°ëª¨ë‹¬ ë¶„ì„',
        isActive: true,
        isDefault: false,
        priority: 2,
        taskTypes: ['vision', 'analysis'],
        maxTokens: 8192,
        temperature: 0.7,
        costPerCredit: 0.00035,
        tier: 'TIER1'
      },
      {
        provider: 'openrouter',
        modelName: 'openai/gpt-3.5-turbo',
        displayName: 'GPT-3.5 Turbo',
        description: 'ê· í˜•ì¡íŒ ì„±ëŠ¥ì˜ ë²”ìš© ëª¨ë¸',
        isActive: true,
        isDefault: false,
        priority: 3,
        taskTypes: ['chat', 'analysis'],
        maxTokens: 4096,
        temperature: 0.7,
        costPerCredit: 0.0015,
        tier: 'TIER2'
      },
      {
        provider: 'openrouter',
        modelName: 'anthropic/claude-3-haiku',
        displayName: 'Claude 3 Haiku',
        description: 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ Claude ëª¨ë¸',
        isActive: true,
        isDefault: false,
        priority: 4,
        taskTypes: ['chat', 'vision', 'analysis'],
        maxTokens: 4096,
        temperature: 0.7,
        costPerCredit: 0.00025,
        tier: 'TIER2'
      },
      {
        provider: 'openrouter',
        modelName: 'openai/gpt-4-turbo',
        displayName: 'GPT-4 Turbo',
        description: 'ìµœê³  ì„±ëŠ¥ì˜ í…ìŠ¤íŠ¸ ë¶„ì„',
        isActive: true,
        isDefault: false,
        priority: 5,
        taskTypes: ['chat', 'analysis'],
        maxTokens: 128000,
        temperature: 0.7,
        costPerCredit: 0.01,
        tier: 'TIER3'
      },
      {
        provider: 'openrouter',
        modelName: 'openai/gpt-4o',
        displayName: 'GPT-4o',
        description: 'ìµœì‹  ë©€í‹°ëª¨ë‹¬ í”Œë˜ê·¸ì‹­ ëª¨ë¸',
        isActive: true,
        isDefault: false,
        priority: 6,
        taskTypes: ['vision', 'analysis'],
        maxTokens: 128000,
        temperature: 0.7,
        costPerCredit: 0.005,
        tier: 'TIER3'
      }
    ];

    for (const model of defaultModels) {
      const apiKey = process.env.OPENROUTER_API_KEY || '';
      const encryptedKey = AIModelManager.encryptApiKey(apiKey);
      
      await prisma.aIModelConfig.upsert({
        where: { modelName: model.modelName },
        update: model,
        create: {
          ...model,
          apiKey: encryptedKey
        }
      });
    }
    
    console.log(`âœ… ${defaultModels.length}ê°œì˜ AI ëª¨ë¸ ì„¤ì • ì™„ë£Œ\n`);

    // 2. ë¼ìš°íŒ… ì •ì±… ìƒì„±
    console.log('ğŸ”€ ë¼ìš°íŒ… ì •ì±… ìƒì„± ì¤‘...');
    
    await prisma.aIModelPolicy.upsert({
      where: { name: 'routing-config' },
      update: {
        isActive: true,
        rules: {
          enableFallback: true,
          fallbackStrategy: 'similar-capability',
          maxRetries: 3,
          costThreshold: 0.01,
          enableCostOptimization: true,
          providerPriority: ['openrouter', 'openai', 'anthropic', 'google'],
          blacklistedModels: [],
          monitoring: {
            alertOnFailure: true,
            alertThreshold: 5
          }
        }
      },
      create: {
        name: 'routing-config',
        description: 'ë™ì  ëª¨ë¸ ë¼ìš°íŒ… ì„¤ì •',
        isActive: true,
        rules: {
          enableFallback: true,
          fallbackStrategy: 'similar-capability',
          maxRetries: 3,
          costThreshold: 0.01,
          enableCostOptimization: true,
          providerPriority: ['openrouter', 'openai', 'anthropic', 'google'],
          blacklistedModels: [],
          monitoring: {
            alertOnFailure: true,
            alertThreshold: 5
          }
        }
      }
    });
    
    console.log('âœ… ë¼ìš°íŒ… ì •ì±… ìƒì„± ì™„ë£Œ\n');

    // 3. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    console.log('ğŸ“ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì¤‘...');
    
    const templates = [
      {
        name: 'Excel ì˜¤ë¥˜ ë¶„ì„ (í•œêµ­ì–´)',
        category: 'error_analysis',
        language: 'ko',
        template: `Excel íŒŒì¼ì—ì„œ {{question}}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ì˜¤ë¥˜ ìœ í˜• ë° ìœ„ì¹˜
   - ì˜¤ë¥˜ ì½”ë“œ/ë©”ì‹œì§€:
   - ë°œìƒ ìœ„ì¹˜ (ì…€/ë²”ìœ„):
   
2. ì˜¤ë¥˜ ì›ì¸ ë¶„ì„
   - ì§ì ‘ì  ì›ì¸:
   - ê·¼ë³¸ ì›ì¸:
   
3. í•´ê²° ë°©ë²•
   - ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…:
   - ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ë²•:
   
4. ì˜ˆë°© ì¡°ì¹˜
   - í–¥í›„ ë°©ì§€ ë°©ë²•:
   - ê¶Œì¥ ì‚¬í•­:`,
        variables: ['question'],
        isActive: true
      },
      {
        name: 'ìˆ˜ì‹ ë„ì›€ë§ (í•œêµ­ì–´)',
        category: 'formula_help',
        language: 'ko',
        template: `ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {{question}}

Excel ìˆ˜ì‹ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. ì¶”ì²œ ìˆ˜ì‹
   \`\`\`
   {{formula}}
   \`\`\`

2. ìˆ˜ì‹ ì„¤ëª…
   - ê° ë¶€ë¶„ì˜ ì—­í• :
   - ì‘ë™ ì›ë¦¬:

3. ì‚¬ìš© ì˜ˆì‹œ
   - ìƒ˜í”Œ ë°ì´í„°:
   - ì˜ˆìƒ ê²°ê³¼:

4. ì£¼ì˜ì‚¬í•­ ë° ëŒ€ì•ˆ
   - í”í•œ ì‹¤ìˆ˜:
   - ëŒ€ì²´ ìˆ˜ì‹:`,
        variables: ['question', 'formula'],
        isActive: true
      }
    ];

    for (const template of templates) {
      await prisma.promptTemplate.create({
        data: {
          ...template,
          examples: JSON.stringify({}),
          performance: JSON.stringify({
            usageCount: 0,
            avgQualityScore: 0,
            avgResponseTime: 0,
            successRate: 0
          })
        }
      });
    }
    
    console.log(`âœ… ${templates.length}ê°œì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ\n`);

    // 4. AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    console.log('ğŸ¯ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...');
    await initializeAISystem();
    console.log('âœ… AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n');

    // 5. í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„± (A/B í…ŒìŠ¤íŒ…ì´ í™œì„±í™”ëœ ê²½ìš°)
    if (process.env.ENABLE_AB_TESTING === 'true') {
      console.log('ğŸ§ª í…ŒìŠ¤íŠ¸ A/B ì‹¤í—˜ ìƒì„± ì¤‘...');
      
      const experiment = await prisma.experiment.create({
        data: {
          name: 'ì´ˆê¸° ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
          description: 'DeepSeek vs GPT-3.5 Turbo ì„±ëŠ¥ ë¹„êµ',
          status: 'active',
          type: 'model',
          variants: JSON.stringify([
            {
              id: 'control',
              name: 'DeepSeek (Control)',
              allocation: 50,
              config: { model: 'deepseek/deepseek-chat' }
            },
            {
              id: 'test',
              name: 'GPT-3.5 Turbo (Test)',
              allocation: 50,
              config: { model: 'openai/gpt-3.5-turbo' }
            }
          ]),
          metrics: ['conversionRate', 'qualityScore', 'responseTime', 'cost'],
          targetAudience: JSON.stringify({ tierRestriction: ['TIER1', 'TIER2'] }),
          startDate: new Date(),
          endDate: new Date(Date.now() + 14 * 24 * 60 * 60 * 1000) // 2ì£¼
        }
      });
      
      console.log(`âœ… A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„± ì™„ë£Œ: ${experiment.name}\n`);
    }

    // 6. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    console.log('ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...');
    
    const modelCount = await prisma.aIModelConfig.count({ where: { isActive: true } });
    const policyCount = await prisma.aIModelPolicy.count({ where: { isActive: true } });
    const templateCount = await prisma.promptTemplate.count({ where: { isActive: true } });
    const experimentCount = await prisma.experiment.count({ where: { status: 'active' } });
    
    console.log(`
ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ:
- í™œì„± AI ëª¨ë¸: ${modelCount}ê°œ
- í™œì„± ì •ì±…: ${policyCount}ê°œ
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ${templateCount}ê°œ
- ì§„í–‰ì¤‘ì¸ ì‹¤í—˜: ${experimentCount}ê°œ
`);

    console.log('âœ¨ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n');
    console.log('ë‹¤ìŒ ë‹¨ê³„:');
    console.log('1. npm run devë¡œ ê°œë°œ ì„œë²„ ì‹œì‘');
    console.log('2. /api/ai/analyze ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì„ ìš”ì²­');
    console.log('3. /api/ai/dashboardë¡œ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ í™•ì¸');
    
  } catch (error) {
    console.error('âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
    process.exit(1);
  } finally {
    await prisma.$disconnect();
  }
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main().catch((error) => {
  console.error('âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨:', error);
  process.exit(1);
});
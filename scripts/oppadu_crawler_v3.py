#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ê²Œì‹œíŒ ìµœì¢… ê°œì„  í¬ë¡¤ëŸ¬ (v3)
- ì™„ì „íˆ ê°œì„ ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ ë¡œì§
- ì •í™•í•œ ì±„íƒëœ ë‹µë³€ë§Œ ì¶”ì¶œ
- ê³ í’ˆì§ˆ Q&A ë°ì´í„° ìƒì„±
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
from urllib.parse import urljoin
import re

class OppaduCrawlerV3:
    def __init__(self):
        self.base_url = "https://www.oppadu.com/community/question/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def get_page_list(self, page_num):
        """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = f"{self.base_url}?board_id=&pg={page_num}"
            print(f"Fetching page {page_num}: {url}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            posts = []
            
            # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                
                if href and '/community/question/' in href and 'no=' in href:
                    full_url = urljoin(self.base_url, href)
                    title = link.get_text(strip=True)
                    
                    if title and len(title) > 5:
                        posts.append({
                            'url': full_url,
                            'title': title
                        })
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_posts = []
            for post in posts:
                if post['url'] not in seen:
                    seen.add(post['url'])
                    unique_posts.append(post)
            
            return unique_posts[:20]  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 20ê°œ
            
        except Exception as e:
            print(f"Error fetching page {page_num}: {e}")
            return []
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ì •ë¦¬í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        
        # 1. HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        text = text.replace('&nbsp;', ' ').replace('&quot;', '"').replace('&#39;', "'")
        
        # 2. HTML íƒœê·¸ ì™„ì „ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # 3. ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\u2640-\u2642" 
            u"\u2600-\u2B55"
            u"\u200d"
            u"\u23cf"
            u"\u23e9"
            u"\u231a"
            u"\ufe0f"  # dingbats
            u"\u3030"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub('', text)
        
        # 4. ì˜¤ë¹ ë‘ íŠ¹í™” ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
        noise_patterns = [
            r'ì˜¤ë¹ ë‘ì—‘ì…€.*?ì»¤ë®¤ë‹ˆí‹°',
            r'ê³µì§€ì‚¬í•­.*?ì‹¤ë¬´ì—‘ì…€',
            r'í˜„ì¬ ì ‘ì†ì.*?ëª…',
            r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼.*?ì¡°íšŒ\s*\d+',
            r'ì—‘ì…€ë²„ì „.*?OSë²„ì „.*?',
            r'ì¢‹ì•„ìš”\d+ëŒ“ê¸€\d+ìŠ¤í¬ë©ê³µìœ ',
            r'ëª©ë¡.*?TOP',
            r'ê²Œì‹œê¸€\s*ëª©ë¡í˜ì´ì§€.*?',
            r'íŒŒì¼\s*ì²¨ë¶€.*?KB',
            r'ë¹„ì£¼ì–¼í…ìŠ¤íŠ¸.*?',
            r'https?://[^\s]+',  # URL
            r'www\.[^\s]+',      # www ë§í¬
            r'ì¢‹ì•„ìš”\s*\d+',
            r'ëŒ“ê¸€\s*\d+',
            r'ìŠ¤í¬ë©',
            r'ì‹ ê³ ',
            r'ì±„íƒëœ\s*ë‹µë³€',
            r'@\w+ë‹˜',
            r'Lv\.\d+',
            r'\d+ì‹œê°„\s*ì „',
            r'\d+ì¼\s*ì „',
            r'\d+ë¶„\s*ì „',
            r'ì‘ì„±ì',
            r'ì§ˆë¬¸ì',
            r'ë‹µë³€ì',
        ]
        
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
        
        # 5. ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n', text)
        
        # 6. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def get_post_content(self, post_url, post_title):
        """ê²Œì‹œê¸€ì—ì„œ ì§ˆë¬¸ê³¼ ì±„íƒëœ ë‹µë³€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            print(f"  Processing: {post_title[:50]}...")
            response = self.session.get(post_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 1. ì§ˆë¬¸ ì œëª© ì¶”ì¶œ
            question_title = ""
            title_selectors = [
                '.answer-complete-text',
                'h1', 'h2', 'h3',
                '.post-title',
                '.board-title'
            ]
            
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem:
                    question_title = self.clean_text(elem.get_text())
                    break
            
            if not question_title:
                question_title = post_title
            
            # 2. ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ
            question_content = ""
            content_selectors = [
                '.post-content',
                '.view-content', 
                '.board-content',
                'article',
                '.question-content'
            ]
            
            for selector in content_selectors:
                elem = soup.select_one(selector)
                if elem:
                    question_content = self.clean_text(elem.get_text())
                    break
            
            # 3. ë²„ì „ ì •ë³´ ì¶”ì¶œ
            version_info = ""
            version_elem = soup.select_one('.post-options-display')
            if version_elem:
                version_info = self.clean_text(version_elem.get_text())
            
            # 4. ì±„íƒëœ ë‹µë³€ë§Œ ì¶”ì¶œ
            selected_answers = []
            
            # ì±„íƒëœ ë‹µë³€ ì„ íƒìë“¤
            answer_selectors = [
                '.comment-wrapper.selected-answer',
                '.selected-answer',
                '.best-answer',
                '.accepted-answer'
            ]
            
            for selector in answer_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    answer_text = self.clean_text(elem.get_text())
                    
                    # í’ˆì§ˆ ê²€ì¦
                    if (answer_text and 
                        len(answer_text) > 30 and 
                        len(answer_text) < 2000 and
                        not self.is_low_quality_answer(answer_text)):
                        selected_answers.append(answer_text)
                
                if selected_answers:
                    break
            
            # ì±„íƒëœ ë‹µë³€ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not selected_answers:
                print(f"    âœ— No selected answers found")
                return None
            
            # 5. ìµœì¢… ê²€ì¦
            final_question = self.build_question(question_title, question_content, version_info)
            final_answer = selected_answers[0]  # ì²« ë²ˆì§¸ ì±„íƒ ë‹µë³€ë§Œ
            
            if (len(final_question) < 20 or 
                len(final_answer) < 30 or
                self.is_low_quality_text(final_question) or
                self.is_low_quality_text(final_answer)):
                print(f"    âœ— Quality check failed")
                return None
            
            return {
                'question': final_question,
                'answer': final_answer
            }
            
        except Exception as e:
            print(f"    âœ— Error: {e}")
            return None
    
    def build_question(self, title, content, version_info):
        """ì§ˆë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        parts = []
        
        if title and len(title) > 5:
            parts.append(f"ì œëª©: {title}")
        
        if content and len(content) > 10:
            parts.append(f"ë‚´ìš©: {content}")
        
        if version_info and len(version_info) > 5:
            parts.append(f"ì‚¬ìš©í™˜ê²½: {version_info}")
        
        return '\n\n'.join(parts)
    
    def is_low_quality_answer(self, text):
        """ì €í’ˆì§ˆ ë‹µë³€ì„ í•„í„°ë§í•©ë‹ˆë‹¤."""
        if not text:
            return True
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‹µë³€
        if len(text) < 30 or len(text) > 2000:
            return True
        
        # ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ë“¤
        low_quality_patterns = [
            r'^[\d\s\.\-=]+$',  # ìˆ«ìì™€ ê¸°í˜¸ë§Œ
            r'^[ê°€-í£]{1,3}$',   # ë„ˆë¬´ ì§§ì€ í•œê¸€
            r'^\w{1,5}$',       # ë„ˆë¬´ ì§§ì€ ì˜ë¬¸
            r'^[\?\!\.\,\s]+$', # íŠ¹ìˆ˜ë¬¸ìë§Œ
        ]
        
        for pattern in low_quality_patterns:
            if re.match(pattern, text):
                return True
        
        return False
    
    def is_low_quality_text(self, text):
        """ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
        if not text:
            return True
        
        # ê¸°ë³¸ í’ˆì§ˆ ê²€ì‚¬
        if len(text.strip()) < 10:
            return True
        
        # ì˜ë¯¸ìˆëŠ” í•œê¸€ì´ë‚˜ ì˜ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if not re.search(r'[ê°€-í£a-zA-Z]', text):
            return True
        
        return False
    
    def save_data(self, data, filename):
        """ë°ì´í„°ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                for item in data:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
            print(f"Saved {len(data)} items to {filename}")
        except Exception as e:
            print(f"Error saving data: {e}")
    
    def crawl_pages(self, start_page=1, end_page=200, output_file='../data/oppadu_high_quality.jsonl'):
        """ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_data = []
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
        
        print(f"=== ì˜¤ë¹ ë‘ ê³ í’ˆì§ˆ í¬ë¡¤ë§ ì‹œì‘: {start_page}-{end_page} í˜ì´ì§€ ===")
        print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        
        for page_num in range(start_page, end_page + 1):
            print(f"\n--- Page {page_num}/{end_page} ---")
            
            posts = self.get_page_list(page_num)
            
            if not posts:
                print(f"No posts found on page {page_num}")
                continue
            
            print(f"Found {len(posts)} posts")
            
            page_success = 0
            for i, post in enumerate(posts):
                qa_data = self.get_post_content(post['url'], post['title'])
                
                if qa_data:
                    all_data.append(qa_data)
                    page_success += 1
                    print(f"    âœ“ Success: {qa_data['question'][:40]}...")
                    
                    # 50ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                    if len(all_data) % 50 == 0:
                        self.save_data(all_data, output_file)
                
                # Rate limiting
                time.sleep(2.5)
            
            print(f"Page {page_num}: {page_success}/{len(posts)} collected")
            
            # í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
            if all_data:
                self.save_data(all_data, output_file)
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            time.sleep(3)
            
            # 10í˜ì´ì§€ë§ˆë‹¤ í†µê³„
            if page_num % 10 == 0:
                print(f"\n=== í†µê³„ (í˜ì´ì§€ {page_num}) ===")
                print(f"ì´ ìˆ˜ì§‘: {len(all_data)}ê°œ")
                print(f"í‰ê· : {len(all_data)/page_num:.1f}ê°œ/í˜ì´ì§€")
                
                if all_data:
                    avg_q_len = sum(len(item['question']) for item in all_data) / len(all_data)
                    avg_a_len = sum(len(item['answer']) for item in all_data) / len(all_data)
                    print(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.0f}ì")
                    print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.0f}ì")
        
        # ìµœì¢… ì €ì¥
        if all_data:
            self.save_data(all_data, output_file)
            
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì´ ìˆ˜ì§‘: {len(all_data)}ê°œ Q&A")
        print(f"íŒŒì¼: {output_file}")
        
        return len(all_data)

def main():
    crawler = OppaduCrawlerV3()
    total_collected = crawler.crawl_pages(1, 300)  # 200 -> 300ìœ¼ë¡œ ë³€ê²½
    print(f"\nìµœì¢… ê²°ê³¼: {total_collected}ê°œ ê³ í’ˆì§ˆ Q&A ìˆ˜ì§‘ ì™„ë£Œ")

if __name__ == "__main__":
    main() 
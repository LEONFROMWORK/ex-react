import { z } from "zod"
import { Result } from "@/Common/Result"
import { OpenAI } from "openai"
import { prisma } from "@/lib/prisma"

// Request/Response types
export class SendChatMessage {
  static readonly Request = z.object({
    userId: z.string(),
    message: z.string().min(1).max(4000),
    context: z.object({
      fileId: z.string().optional(),
      analysisId: z.string().optional(),
      conversationId: z.string().optional(),
    }).optional(),
    preferredTier: z.enum(["ECONOMY", "PREMIUM"]).default("ECONOMY"),
  })

  static readonly Response = z.object({
    conversationId: z.string(),
    messageId: z.string(),
    response: z.string(),
    tokensUsed: z.number(),
    estimatedCost: z.number(),
    aiTier: z.enum(["TIER1", "TIER2"]),
    suggestions: z.array(z.string()).optional(),
    attachments: z.array(z.object({
      type: z.enum(["excel_template", "code_snippet", "chart"]),
      content: z.any(),
    })).optional(),
  })
}

export type SendChatMessageRequest = z.infer<typeof SendChatMessage.Request>
export type SendChatMessageResponse = z.infer<typeof SendChatMessage.Response>

// Validator
export class SendChatMessageValidator {
  static validate(request: unknown): Result<SendChatMessageRequest> {
    try {
      const validated = SendChatMessage.Request.parse(request)
      return Result.success(validated)
    } catch (error) {
      if (error instanceof z.ZodError) {
        return Result.failure({
          code: "VALIDATION_ERROR",
          message: error.errors.map(e => e.message).join(", "),
        })
      }
      return Result.failure({
        code: "INVALID_REQUEST",
        message: "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤.",
      })
    }
  }
}

// Intent classifier for 2-tier AI system
export class ChatIntentClassifier {
  private static readonly SIMPLE_PATTERNS = [
    { pattern: /í…œí”Œë¦¿|template|ì–‘ì‹/i, category: "simple_template" },
    { pattern: /ê°„ë‹¨í•œ|ê¸°ë³¸|basic|simple/i, category: "simple_template" },
    { pattern: /ë§Œë“¤ì–´|ìƒì„±|create|make/i, category: "simple_template" },
  ]

  private static readonly COMPLEX_INDICATORS = [
    /ë³µì¡í•œ|complex|advanced/i,
    /ë§ì¶¤|custom|personalized/i,
    /ë¶„ì„|analysis|analyze/i,
    /ìµœì í™”|optimize/i,
  ]

  static classifyIntent(message: string): "simple" | "complex" | "clarification_needed" {
    // Check for simple patterns
    const isSimple = this.SIMPLE_PATTERNS.some(({ pattern }) => 
      pattern.test(message)
    )
    
    // Check for complex indicators
    const isComplex = this.COMPLEX_INDICATORS.some(pattern => 
      pattern.test(message)
    )
    
    if (isSimple && !isComplex) return "simple"
    if (isComplex) return "complex"
    
    // Default to clarification if unclear
    return message.length < 20 ? "clarification_needed" : "complex"
  }
}

// Handler
export class SendChatMessageHandler {
  private openai: OpenAI
  private readonly TIER1_MODEL = "gpt-3.5-turbo"
  private readonly TIER2_MODEL = "gpt-4"
  private readonly TIER1_COST_PER_TOKEN = 0.0005
  private readonly TIER2_COST_PER_TOKEN = 0.03

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    })
  }

  async handle(request: SendChatMessageRequest): Promise<Result<SendChatMessageResponse>> {
    try {
      // Get or create conversation
      const conversationId = request.context?.conversationId || 
        await this.createNewConversation(request.userId)

      // Classify intent
      const intent = ChatIntentClassifier.classifyIntent(request.message)

      // Route to appropriate tier
      let response: SendChatMessageResponse
      
      if (intent === "simple" || request.preferredTier === "ECONOMY") {
        response = await this.handleTier1Request(request, conversationId)
      } else if (intent === "complex" || request.preferredTier === "PREMIUM") {
        response = await this.handleTier2Request(request, conversationId)
      } else {
        // Clarification needed
        response = await this.handleClarificationRequest(request, conversationId)
      }

      // Save to conversation history
      await this.saveToHistory(request, response)

      return Result.success(response)
    } catch (error) {
      console.error("Chat error:", error)
      return Result.failure({
        code: "CHAT_FAILED",
        message: "ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
      })
    }
  }

  private async handleTier1Request(
    request: SendChatMessageRequest, 
    conversationId: string
  ): Promise<SendChatMessageResponse> {
    const systemPrompt = this.buildTier1SystemPrompt()
    const userPrompt = await this.buildUserPrompt(request)

    const completion = await this.openai.chat.completions.create({
      model: this.TIER1_MODEL,
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      temperature: 0.7,
      max_tokens: 1000,
    })

    const response = completion.choices[0].message.content || ""
    const tokensUsed = completion.usage?.total_tokens || 0

    // Check if escalation to Tier 2 is needed
    if (this.shouldEscalateToTier2(response)) {
      return this.handleTier2Request(request, conversationId)
    }

    return {
      conversationId,
      messageId: crypto.randomUUID(),
      response,
      tokensUsed,
      estimatedCost: tokensUsed * this.TIER1_COST_PER_TOKEN,
      aiTier: "TIER1",
      suggestions: this.extractSuggestions(response),
    }
  }

  private async handleTier2Request(
    request: SendChatMessageRequest,
    conversationId: string
  ): Promise<SendChatMessageResponse> {
    const systemPrompt = this.buildTier2SystemPrompt()
    const userPrompt = await this.buildUserPrompt(request)

    const completion = await this.openai.chat.completions.create({
      model: this.TIER2_MODEL,
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      temperature: 0.7,
      max_tokens: 2000,
    })

    const response = completion.choices[0].message.content || ""
    const tokensUsed = completion.usage?.total_tokens || 0

    return {
      conversationId,
      messageId: crypto.randomUUID(),
      response,
      tokensUsed,
      estimatedCost: tokensUsed * this.TIER2_COST_PER_TOKEN,
      aiTier: "TIER2",
      suggestions: this.extractSuggestions(response),
      attachments: await this.generateAttachments(response, request),
    }
  }

  private async handleClarificationRequest(
    request: SendChatMessageRequest,
    conversationId: string
  ): Promise<SendChatMessageResponse> {
    const clarificationPrompt = `
ì‚¬ìš©ìì˜ ìš”ì²­ì´ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒ ì˜µì…˜ ì¤‘ ì„ íƒí•˜ë„ë¡ ì•ˆë‚´í•´ì£¼ì„¸ìš”:

1. ğŸ“Š ê¸°ë³¸ í…œí”Œë¦¿ (ì¦‰ì‹œ ìƒì„± ê°€ëŠ¥)
   - ë¯¸ë¦¬ ì¤€ë¹„ëœ í…œí”Œë¦¿ ì‚¬ìš©
   - ë¹ ë¥¸ ìƒì„±
   - ê¸°ë³¸ ê¸°ëŠ¥ë§Œ í¬í•¨

2. ğŸ”· ë§ì¶¤í˜• í…œí”Œë¦¿ (ê³ ê¸‰ AI í•„ìš”)
   - ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ì¶¤
   - ë³µì¡í•œ ê¸°ëŠ¥ í¬í•¨
   - ì¶”ê°€ ë¹„ìš© ë°œìƒ

ì‚¬ìš©ì ë©”ì‹œì§€: "${request.message}"
`

    return {
      conversationId,
      messageId: crypto.randomUUID(),
      response: clarificationPrompt,
      tokensUsed: 0,
      estimatedCost: 0,
      aiTier: "TIER1",
      suggestions: ["ê¸°ë³¸ í…œí”Œë¦¿ ì„ íƒ", "ë§ì¶¤í˜• í…œí”Œë¦¿ ì„ íƒ"],
    }
  }

  private buildTier1SystemPrompt(): string {
    return `
ë‹¹ì‹ ì€ Excel ë¬¸ì„œ ìƒì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê°„ë‹¨í•œ í…œí”Œë¦¿ê³¼ ê¸°ë³¸ì ì¸ Excel ê¸°ëŠ¥ì— ëŒ€í•´ ì•ˆë‚´í•©ë‹ˆë‹¤.
ë³µì¡í•œ ìš”ì²­ì€ ê³ ê¸‰ AIê°€ í•„ìš”í•¨ì„ ì•ˆë‚´í•˜ì„¸ìš”.

ì‘ë‹µ ê·œì¹™:
1. ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€
2. ê¸°ë³¸ í…œí”Œë¦¿ ì œê³µ ê°€ëŠ¥
3. ë³µì¡í•œ ìš”ì²­ì‹œ "ê³ ê¸‰ AI ë¶„ì„ í•„ìš”" ëª…ì‹œ
4. í•œêµ­ì–´ë¡œ ì‘ë‹µ
`
  }

  private buildTier2SystemPrompt(): string {
    return `
ë‹¹ì‹ ì€ ê³ ê¸‰ Excel ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.
ë³µì¡í•œ Excel ë¬¸ì„œ ìƒì„±, ê³ ê¸‰ ìˆ˜ì‹, ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì‘ë‹µ ê·œì¹™:
1. ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€
2. ë§ì¶¤í˜• ì†”ë£¨ì…˜ ì œê³µ
3. ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë‚˜ ìˆ˜ì‹ í¬í•¨
4. ìµœì í™” ì œì•ˆ í¬í•¨
5. í•œêµ­ì–´ë¡œ ì‘ë‹µ
`
  }

  private async buildUserPrompt(request: SendChatMessageRequest): Promise<string> {
    let prompt = request.message

    // Add context if available
    if (request.context?.fileId) {
      const file = await prisma.file.findUnique({
        where: { id: request.context.fileId },
        include: { analyses: { take: 1 } },
      })
      
      if (file) {
        prompt += `\n\níŒŒì¼ ì»¨í…ìŠ¤íŠ¸: ${file.originalName}`
        if (file.analyses[0]) {
          prompt += `\në°œê²¬ëœ ì˜¤ë¥˜: ${(file.analyses[0].report as any).totalErrors}ê°œ`
        }
      }
    }

    return prompt
  }

  private shouldEscalateToTier2(response: string): boolean {
    const escalationIndicators = [
      "ê³ ê¸‰ AI ë¶„ì„ì´ í•„ìš”",
      "ë³µì¡í•œ ìš”ì²­",
      "ì¶”ê°€ ë¶„ì„ í•„ìš”",
    ]
    
    return escalationIndicators.some(indicator => 
      response.includes(indicator)
    )
  }

  private extractSuggestions(response: string): string[] {
    const suggestions: string[] = []
    
    // Extract numbered suggestions
    const numberPattern = /\d+\.\s*([^\n]+)/g
    let match
    while ((match = numberPattern.exec(response)) !== null) {
      suggestions.push(match[1].trim())
    }
    
    return suggestions.slice(0, 3) // Return top 3 suggestions
  }

  private async generateAttachments(
    response: string,
    request: SendChatMessageRequest
  ): Promise<any[]> {
    const attachments = []
    
    // Check if response mentions creating a template
    if (response.includes("í…œí”Œë¦¿") || response.includes("template")) {
      attachments.push({
        type: "excel_template",
        content: {
          name: "generated-template.xlsx",
          description: "AIê°€ ìƒì„±í•œ Excel í…œí”Œë¦¿",
        },
      })
    }
    
    return attachments
  }

  private async createNewConversation(userId: string): Promise<string> {
    const conversation = await prisma.chatConversation.create({
      data: {
        userId,
        title: "ìƒˆ ëŒ€í™”",
      },
    })
    
    return conversation.id
  }

  private async saveToHistory(
    request: SendChatMessageRequest,
    response: SendChatMessageResponse
  ): Promise<void> {
    await prisma.chatMessage.createMany({
      data: [
        {
          conversationId: response.conversationId,
          role: "user",
          content: request.message,
        },
        {
          conversationId: response.conversationId,
          role: "assistant",
          content: response.response,
          tokensUsed: response.tokensUsed,
          aiTier: response.aiTier,
        },
      ],
    })
  }
}
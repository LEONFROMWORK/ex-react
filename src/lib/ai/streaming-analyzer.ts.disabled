/**
 * ì‹¤ì‹œê°„ AI ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œìŠ¤í…œ
 * 30ì´ˆ ëŒ€ê¸° â†’ 3ì´ˆ ì‹¤ì‹œê°„ ì‘ë‹µìœ¼ë¡œ ê°œì„ 
 */

import { TierSystemManager } from './tier-system'
import { OpenRouterProvider } from './providers/openrouter'
import { AIOptions, AIResponse } from './types'

export interface StreamingAnalysisResult {
  partialResults: PartialResult[]
  finalResult: AIResponse
  streamingStats: {
    totalTime: number
    firstResponseTime: number
    chunksReceived: number
    averageChunkTime: number
  }
}

export interface PartialResult {
  content: string
  confidence: number
  tier: number
  timestamp: number
  isComplete: boolean
  suggestions?: string[]
}

export interface StreamingOptions extends AIOptions {
  enablePartialResults?: boolean
  chunkCallback?: (chunk: PartialResult) => void
  confidenceThreshold?: number
  maxStreamTime?: number
}

export class StreamingAIAnalyzer {
  private tierSystem: TierSystemManager
  
  constructor(apiKey: string) {
    this.tierSystem = new TierSystemManager(apiKey)
  }

  async analyzeWithStreaming(
    prompt: string,
    options: StreamingOptions = {}
  ): Promise<StreamingAnalysisResult> {
    console.log('ğŸŒŠ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œì‘')
    
    const startTime = performance.now()
    const partialResults: PartialResult[] = []
    let firstResponseTime = 0
    let chunksReceived = 0
    
    // ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
    const streamOptions: StreamingOptions = {
      enablePartialResults: true,
      confidenceThreshold: 0.7,
      maxStreamTime: 30000, // 30ì´ˆ ìµœëŒ€ ëŒ€ê¸°
      ...options
      // stream: true // OpenRouter ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” (íƒ€ì… ì˜¤ë¥˜ë¡œ ì„ì‹œ ì œê±°)
    }

    try {
      // Tier ì„ íƒ (ë³µì¡ë„ ê¸°ë°˜ ì‚¬ì „ íŒë‹¨)
      const complexity = this.analyzeComplexity(prompt)
      const startTier = this.selectStartingTier(complexity)
      
      console.log(`ğŸ¯ ì‹œì‘ Tier: ${startTier} (ë³µì¡ë„: ${complexity})`)
      
      // ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹¤í–‰
      const result = await this.streamingAnalysis(prompt, startTier, streamOptions, {
        onPartialResult: (partial) => {
          if (firstResponseTime === 0) {
            firstResponseTime = performance.now() - startTime
            console.log(`âš¡ ì²« ì‘ë‹µ: ${firstResponseTime.toFixed(2)}ms`)
          }
          
          chunksReceived++
          partialResults.push(partial)
          
          // ì‹¤ì‹œê°„ ì½œë°± í˜¸ì¶œ
          if (options.chunkCallback) {
            options.chunkCallback(partial)
          }
          
          console.log(`ğŸ“¦ ì²­í¬ ${chunksReceived}: ${partial.content.slice(0, 50)}...`)
        }
      })
      
      const totalTime = performance.now() - startTime
      
      return {
        partialResults,
        finalResult: result,
        streamingStats: {
          totalTime,
          firstResponseTime,
          chunksReceived,
          averageChunkTime: chunksReceived > 0 ? totalTime / chunksReceived : 0
        }
      }
      
    } catch (error) {
      console.error('âŒ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹¤íŒ¨:', error)
      throw error
    }
  }

  private async streamingAnalysis(
    prompt: string,
    tier: number,
    options: StreamingOptions,
    callbacks: {
      onPartialResult: (result: PartialResult) => void
    }
  ): Promise<AIResponse> {
    const tierConfig = this.tierSystem.getTierInfo()[tier - 1]
    const provider = new OpenRouterProvider(process.env.OPENROUTER_API_KEY!, tierConfig.model)
    
    // ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ìƒì„±
    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
        'HTTP-Referer': process.env.NEXT_PUBLIC_SITE_URL || 'https://exhell.app',
        'X-Title': 'Exhell Excel Assistant',
      },
      body: JSON.stringify({
        model: tierConfig.model,
        messages: [
          {
            role: 'system',
            content: this.buildStreamingPrompt(tier, options.systemPrompt)
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        // stream: true // íƒ€ì… ì˜¤ë¥˜ë¡œ ì„ì‹œ ì œê±°,
        max_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
      })
    })

    if (!response.ok) {
      throw new Error(`ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì‹¤íŒ¨: ${response.status}`)
    }

    return await this.processStreamingResponse(response, tier, callbacks)
  }

  private async processStreamingResponse(
    response: Response,
    tier: number,
    callbacks: {
      onPartialResult: (result: PartialResult) => void
    }
  ): Promise<AIResponse> {
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error('ìŠ¤íŠ¸ë¦¼ ë¦¬ë” ìƒì„± ì‹¤íŒ¨')
    }

    const decoder = new TextDecoder()
    let buffer = ''
    let fullContent = ''
    let currentConfidence = 0

    try {
      while (true) {
        const { done, value } = await reader.read()
        
        if (done) break
        
        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ''
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)
            
            if (data === '[DONE]') {
              continue
            }
            
            try {
              const parsed = JSON.parse(data)
              const delta = parsed.choices?.[0]?.delta?.content
              
              if (delta) {
                fullContent += delta
                currentConfidence = this.estimateStreamingConfidence(fullContent)
                
                // ë¶€ë¶„ ê²°ê³¼ ìƒì„±
                const partialResult: PartialResult = {
                  content: fullContent,
                  confidence: currentConfidence,
                  tier,
                  timestamp: Date.now(),
                  isComplete: false,
                  suggestions: this.extractSuggestions(fullContent)
                }
                
                callbacks.onPartialResult(partialResult)
              }
              
            } catch (error) {
              console.warn('ìŠ¤íŠ¸ë¦¼ íŒŒì‹± ì˜¤ë¥˜:', error)
            }
          }
        }
      }
      
      // ìµœì¢… ê²°ê³¼ ìƒì„±
      const finalConfidence = this.extractFinalConfidence(fullContent)
      
      // ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì—ìŠ¤ì»¬ë ˆì´ì…˜
      if (finalConfidence < 0.85 && tier < 3) {
        console.log(`â¬†ï¸ Tier ${tier} â†’ ${tier + 1} ì—ìŠ¤ì»¬ë ˆì´ì…˜ (ì‹ ë¢°ë„: ${finalConfidence})`)
        return await this.streamingAnalysis(
          `ì´ì „ ë¶„ì„ ê²°ê³¼: ${fullContent}\n\në” ì •í™•í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.`,
          tier + 1,
          // { stream: true }, // íƒ€ì… ì˜¤ë¥˜ë¡œ ì„ì‹œ ì œê±°
          callbacks
        )
      }
      
      return {
        content: fullContent,
        model: this.tierSystem.getTierInfo()[tier - 1].model,
        provider: 'openrouter',
        usage: {
          promptTokens: 0, // ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ì¶”ì •ê°’
          completionTokens: this.estimateTokens(fullContent),
          totalTokens: 0
        },
        latency: 0,
        cost: 0
      }
      
    } finally {
      reader.releaseLock()
    }
  }

  private buildStreamingPrompt(tier: number, systemPrompt?: string): string {
    const tierPrompts = {
      1: `ë‹¹ì‹ ì€ ë¹ ë¥¸ Excel ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
           ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.
           ë¨¼ì € í•µì‹¬ ë¬¸ì œë¥¼ íŒŒì•…í•˜ê³ , ì ì§„ì ìœ¼ë¡œ ìƒì„¸í•œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”.
           ê° ë‹¨ê³„ë§ˆë‹¤ í˜„ì¬ ì‹ ë¢°ë„ë¥¼ "ì‹ ë¢°ë„: 0.XX"ë¡œ í‘œì‹œí•˜ì„¸ìš”.`,
      
      2: `ë‹¹ì‹ ì€ ë³µì¡í•œ Excel ë¬¸ì œ í•´ê²° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
           ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¨ê³„ë³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
           1ë‹¨ê³„: ë¬¸ì œ ì‹ë³„, 2ë‹¨ê³„: ì›ì¸ ë¶„ì„, 3ë‹¨ê³„: í•´ê²°ì±… ì œì‹œ
           ê° ë‹¨ê³„ë§ˆë‹¤ ì‹ ë¢°ë„ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.`,
      
      3: `ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ Excel ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
           ê°€ì¥ ë³µì¡í•œ ë¬¸ì œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í•´ê²°í•˜ì„¸ìš”.
           í¬ê´„ì ì¸ ë¶„ì„ê³¼ ë‹¤ì–‘í•œ í•´ê²° ë°©ì•ˆì„ ë‹¨ê³„ë³„ë¡œ ì œì‹œí•˜ì„¸ìš”.
           ìµœì¢… ì‹ ë¢°ë„ë¥¼ "ìµœì¢… ì‹ ë¢°ë„: 0.XX"ë¡œ í‘œì‹œí•˜ì„¸ìš”.`
    }
    
    const tierPrompt = tierPrompts[tier as keyof typeof tierPrompts] || tierPrompts[1]
    
    return systemPrompt 
      ? `${systemPrompt}\n\n${tierPrompt}`
      : tierPrompt
  }

  private analyzeComplexity(prompt: string): number {
    let complexity = 0.3 // ê¸°ë³¸ ë³µì¡ë„
    
    // í‚¤ì›Œë“œ ê¸°ë°˜ ë³µì¡ë„ ë¶„ì„
    const complexKeywords = [
      'vlookup', 'index', 'match', 'array', 'pivot', 'macro', 'vba',
      'ìˆœí™˜ì°¸ì¡°', 'ë³µì¡í•œ', 'ì—¬ëŸ¬', 'ë‹¤ì¤‘', 'ê³ ê¸‰', 'ìµœì í™”'
    ]
    
    const advancedKeywords = [
      'power query', 'power pivot', 'dynamic array', 'ëŒë‹¤',
      'ì—°ê²°ëœ ë°ì´í„°', 'ì™¸ë¶€ ë°ì´í„°', 'api', 'json', 'xml'
    ]
    
    // ê¸°ë³¸ ë³µì¡ë„ ê³„ì‚°
    complexKeywords.forEach(keyword => {
      if (prompt.toLowerCase().includes(keyword)) {
        complexity += 0.15
      }
    })
    
    // ê³ ê¸‰ ë³µì¡ë„ ê³„ì‚°
    advancedKeywords.forEach(keyword => {
      if (prompt.toLowerCase().includes(keyword)) {
        complexity += 0.25
      }
    })
    
    // ë¬¸ì„œ ê¸¸ì´ ê¸°ë°˜ ë³µì¡ë„
    if (prompt.length > 500) complexity += 0.1
    if (prompt.length > 1000) complexity += 0.1
    
    return Math.min(complexity, 1.0)
  }

  private selectStartingTier(complexity: number): number {
    if (complexity > 0.8) return 3 // ê³ ë³µì¡ë„ â†’ Tier 3
    if (complexity > 0.5) return 2 // ì¤‘ë³µì¡ë„ â†’ Tier 2
    return 1 // ì €ë³µì¡ë„ â†’ Tier 1
  }

  private estimateStreamingConfidence(content: string): number {
    // ì‹¤ì‹œê°„ ì‹ ë¢°ë„ ì¶”ì •
    let confidence = 0.3
    
    // ë‚´ìš© ê¸¸ì´ ê¸°ë°˜
    if (content.length > 100) confidence += 0.2
    if (content.length > 300) confidence += 0.2
    
    // Excel ì „ë¬¸ ìš©ì–´ í¬í•¨ë„
    const terms = ['ìˆ˜ì‹', 'ì…€', 'í•¨ìˆ˜', 'ë²”ìœ„', 'ì›Œí¬ì‹œíŠ¸']
    const termCount = terms.filter(term => content.includes(term)).length
    confidence += (termCount / terms.length) * 0.3
    
    // êµ¬ì¡°í™”ëœ ë‹µë³€ ì—¬ë¶€
    if (content.includes('1.') || content.includes('ë‹¨ê³„')) confidence += 0.1
    if (content.includes('í•´ê²°ì±…') || content.includes('ë°©ë²•')) confidence += 0.1
    
    return Math.min(confidence, 0.95)
  }

  private extractFinalConfidence(content: string): number {
    const matches = content.match(/(?:ìµœì¢…\s*)?ì‹ ë¢°ë„:\s*(\d*\.?\d+)/i)
    if (matches) {
      return parseFloat(matches[1])
    }
    return this.estimateStreamingConfidence(content)
  }

  private extractSuggestions(content: string): string[] {
    const suggestions: string[] = []
    
    // ì¼ë°˜ì ì¸ ì œì•ˆ íŒ¨í„´ ì¶”ì¶œ
    const suggestionPatterns = [
      /(\d+\.\s*[^.]+(?:í•˜ì„¸ìš”|í•´ë³´ì„¸ìš”|ê¶Œì¥í•©ë‹ˆë‹¤))/g,
      /(ê¶Œì¥.*?[:ï¼š]\s*[^.]+)/g,
      /(í•´ê²°ì±….*?[:ï¼š]\s*[^.]+)/g
    ]
    
    suggestionPatterns.forEach(pattern => {
      const matches = content.match(pattern)
      if (matches) {
        suggestions.push(...matches.slice(0, 3)) // ìµœëŒ€ 3ê°œ
      }
    })
    
    return suggestions.slice(0, 5) // ìµœëŒ€ 5ê°œ ì œì•ˆ
  }

  private estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
    return Math.ceil(text.length / 4)
  }

  // ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
  async benchmarkStreaming(prompt: string): Promise<{
    streaming: number
    traditional: number
    improvement: number
  }> {
    console.log('ğŸ“Š ìŠ¤íŠ¸ë¦¬ë° vs ì „í†µì  ë°©ì‹ ë²¤ì¹˜ë§ˆí¬')
    
    // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì¸¡ì •
    const streamingStart = performance.now()
    const streamingResult = await this.analyzeWithStreaming(prompt)
    const streamingTime = streamingResult.streamingStats.firstResponseTime
    
    // ì „í†µì  ë°©ì‹ ì¸¡ì • (ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”)
    const traditionalStart = performance.now()
    await this.tierSystem.analyzeWithTiers(prompt, { stream: false })
    const traditionalTime = performance.now() - traditionalStart
    
    const improvement = ((traditionalTime - streamingTime) / traditionalTime) * 100
    
    console.log(`ğŸ“ˆ ì„±ëŠ¥ ê°œì„ : ${improvement.toFixed(1)}% (${traditionalTime.toFixed(0)}ms â†’ ${streamingTime.toFixed(0)}ms)`)
    
    return {
      streaming: streamingTime,
      traditional: traditionalTime,
      improvement
    }
  }
}

// ì‚¬ìš© ì˜ˆì‹œ
export async function analyzeExcelWithStreaming(
  prompt: string,
  options: StreamingOptions = {}
): Promise<StreamingAnalysisResult> {
  const analyzer = new StreamingAIAnalyzer(process.env.OPENROUTER_API_KEY!)
  return await analyzer.analyzeWithStreaming(prompt, options)
}
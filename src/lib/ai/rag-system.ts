/**
 * pgvector ê¸°ë°˜ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
 * Neon PostgreSQL pgvectorë¡œ ë²¡í„° DB í†µí•©, 79% ë¹„ìš© ì ˆê°
 */

import { prisma } from '../prisma'
import { StreamingAIAnalyzer } from './streaming-analyzer'

export interface RAGSearchResult {
  id: number
  content: string
  similarity: number
  metadata: {
    errorType: string
    solution: string
    confidence: number
    usageCount: number
    lastUsed: Date
  }
}

export interface RAGEnhancedResponse {
  response: string
  sources: RAGSearchResult[]
  confidence: number
  costSaved: number
  cacheHit: boolean
}

export interface KnowledgeEntry {
  content: string
  embedding: number[]
  metadata: {
    category: string
    tags: string[]
    difficulty: number
    effectiveness: number
  }
}

export class RAGSystem {
  private streamingAnalyzer: StreamingAIAnalyzer
  private embeddingCache = new Map<string, number[]>()
  
  constructor(apiKey: string) {
    this.streamingAnalyzer = new StreamingAIAnalyzer(apiKey)
  }

  async searchSimilarSolutions(
    query: string,
    options: {
      limit?: number
      threshold?: number
      categories?: string[]
    } = {}
  ): Promise<RAGSearchResult[]> {
    console.log('ğŸ” ìœ ì‚¬ í•´ê²°ì±… ê²€ìƒ‰ ì‹œì‘')
    
    const { limit = 10, threshold = 0.8, categories } = options
    
    // ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    const queryEmbedding = await this.generateEmbedding(query)
    
    // pgvector ìœ ì‚¬ì„± ê²€ìƒ‰ (Neon PostgreSQL)
    const results = await prisma.$queryRaw`
      SELECT 
        id,
        content,
        metadata,
        1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) as similarity
      FROM knowledge_base
      WHERE 
        1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) > ${threshold}
        ${categories ? `AND metadata->>'category' = ANY(${categories})` : ''}
      ORDER BY embedding <=> ${JSON.stringify(queryEmbedding)}::vector
      LIMIT ${limit}
    ` as any[]
    
    console.log(`ğŸ“Š ${results.length}ê°œ ìœ ì‚¬ í•´ê²°ì±… ë°œê²¬`)
    
    return results.map(row => ({
      id: row.id,
      content: row.content,
      similarity: row.similarity,
      metadata: {
        errorType: row.metadata.errorType || 'unknown',
        solution: row.metadata.solution || '',
        confidence: row.metadata.confidence || 0,
        usageCount: row.metadata.usageCount || 0,
        lastUsed: row.metadata.lastUsed ? new Date(row.metadata.lastUsed) : new Date()
      }
    }))
  }

  async enhanceWithRAG(
    prompt: string,
    options: {
      useCache?: boolean
      maxSources?: number
      confidenceBoost?: number
    } = {}
  ): Promise<RAGEnhancedResponse> {
    console.log('ğŸš€ RAG ê°•í™” ë¶„ì„ ì‹œì‘')
    
    const { useCache = true, maxSources = 5, confidenceBoost = 0.1 } = options
    const startTime = performance.now()
    
    // 1. ìºì‹œ í™•ì¸
    if (useCache) {
      const cachedResponse = await this.checkCache(prompt)
      if (cachedResponse) {
        console.log('âš¡ ìºì‹œ íˆíŠ¸!')
        return {
          ...cachedResponse,
          cacheHit: true,
          costSaved: this.estimateCostSaved(prompt)
        }
      }
    }
    
    // 2. ìœ ì‚¬ í•´ê²°ì±… ê²€ìƒ‰
    const similarSolutions = await this.searchSimilarSolutions(prompt, {
      limit: maxSources,
      threshold: 0.75
    })
    
    // 3. ì»¨í…ìŠ¤íŠ¸ ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
    const enhancedPrompt = this.buildRAGPrompt(prompt, similarSolutions)
    
    // 4. AI ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë°)
    const streamingResult = await this.streamingAnalyzer.analyzeWithStreaming(
      enhancedPrompt,
      {
        systemPrompt: this.buildRAGSystemPrompt(),
        chunkCallback: (chunk) => {
          console.log(`ğŸ“¦ RAG ì²­í¬: ${chunk.content.slice(0, 30)}...`)
        }
      }
    )
    
    const response = streamingResult.finalResult.content
    const confidence = this.extractConfidence(response) + confidenceBoost
    
    // 5. ì‘ë‹µ ìºì‹±
    if (useCache && confidence > 0.8) {
      await this.cacheResponse(prompt, response, confidence, similarSolutions)
    }
    
    // 6. ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
    await this.updateKnowledgeBase(prompt, response, confidence)
    
    const processingTime = performance.now() - startTime
    console.log(`âœ… RAG ê°•í™” ì™„ë£Œ: ${processingTime.toFixed(2)}ms`)
    
    return {
      response,
      sources: similarSolutions,
      confidence,
      costSaved: this.estimateCostSaved(prompt, similarSolutions.length),
      cacheHit: false
    }
  }

  private async checkCache(prompt: string): Promise<RAGEnhancedResponse | null> {
    const promptHash = await this.hashPrompt(prompt)
    const promptEmbedding = await this.generateEmbedding(prompt)
    
    // ìœ ì‚¬í•œ ìºì‹œëœ ì‘ë‹µ ê²€ìƒ‰
    const cachedResults = await prisma.$queryRaw`
      SELECT 
        response_content,
        confidence_score,
        cost_saved,
        sources_used,
        hit_count
      FROM ai_response_cache
      WHERE 
        1 - (prompt_embedding <=> ${JSON.stringify(promptEmbedding)}::vector) > 0.95
      ORDER BY 
        1 - (prompt_embedding <=> ${JSON.stringify(promptEmbedding)}::vector) DESC,
        hit_count DESC
      LIMIT 1
    ` as any[]
    
    if (cachedResults.length > 0) {
      const cached = cachedResults[0]
      
      // ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
      await prisma.$executeRaw`
        UPDATE ai_response_cache 
        SET 
          hit_count = hit_count + 1,
          last_used = now()
        WHERE prompt_hash = ${promptHash}
      `
      
      return {
        response: cached.response_content,
        sources: cached.sources_used || [],
        confidence: cached.confidence_score,
        costSaved: cached.cost_saved,
        cacheHit: true
      }
    }
    
    return null
  }

  private buildRAGPrompt(originalPrompt: string, sources: RAGSearchResult[]): string {
    if (sources.length === 0) {
      return originalPrompt
    }
    
    const contextSections = sources.map((source, index) => {
      return `
**ì°¸ì¡° ${index + 1}** (ìœ ì‚¬ë„: ${(source.similarity * 100).toFixed(1)}%)
ë¬¸ì œ: ${source.metadata.errorType}
í•´ê²°ì±…: ${source.metadata.solution}
ìƒì„¸: ${source.content.slice(0, 200)}...
ì‹ ë¢°ë„: ${source.metadata.confidence}
ì‚¬ìš© íšŸìˆ˜: ${source.metadata.usageCount}
      `.trim()
    }).join('\n\n')
    
    return `
ë‹¤ìŒì€ ìœ ì‚¬í•œ Excel ë¬¸ì œë“¤ê³¼ í•´ê²°ì±…ì…ë‹ˆë‹¤:

${contextSections}

---

ìœ„ì˜ ì°¸ì¡° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”:

${originalPrompt}

**ì¤‘ìš”**: 
1. ì°¸ì¡° ì •ë³´ë¥¼ í™œìš©í•˜ë˜, í˜„ì¬ ë¬¸ì œì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”
2. ì°¸ì¡°í•œ í•´ê²°ì±…ì˜ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "ì°¸ì¡° 1ì˜ ë°©ë²•ì„ ì‘ìš©í•˜ë©´...")
3. ê¸°ì¡´ í•´ê²°ì±…ë³´ë‹¤ ê°œì„ ëœ ë°©ì•ˆì´ ìˆë‹¤ë©´ ì œì‹œí•˜ì„¸ìš”
4. ì‹ ë¢°ë„ë¥¼ "ì‹ ë¢°ë„: 0.XX" í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
    `.trim()
  }

  private buildRAGSystemPrompt(): string {
    return `
ë‹¹ì‹ ì€ Excel ì „ë¬¸ê°€ì´ë©°, ê³¼ê±° í•´ê²° ì‚¬ë¡€ë¥¼ ì°¸ì¡°í•˜ì—¬ ë” ì •í™•í•˜ê³  íš¨ê³¼ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤.

**ì—­í• **:
1. ì œê³µëœ ì°¸ì¡° ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ íŒŒì•…
2. í˜„ì¬ ë¬¸ì œì— ê°€ì¥ ì í•©í•œ í•´ê²° ë°©ë²• ì„ íƒ
3. ì°¸ì¡° ì‚¬ë¡€ë³´ë‹¤ ê°œì„ ëœ ë°©ì•ˆ ì œì‹œ
4. ì‹¤ë¬´ì—ì„œ ê²€ì¦ëœ ë°©ë²• ìš°ì„  í™œìš©

**ì‘ë‹µ í˜•ì‹**:
1. ë¬¸ì œ ë¶„ì„ ë° ì°¸ì¡° ì‚¬ë¡€ í™œìš©
2. ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•
3. ì¶”ê°€ ìµœì í™” ë°©ì•ˆ
4. ì‹ ë¢°ë„ ì ìˆ˜

ì°¸ì¡° ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë†’ì€ í’ˆì§ˆì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    `.trim()
  }

  private async cacheResponse(
    prompt: string,
    response: string,
    confidence: number,
    sources: RAGSearchResult[]
  ): Promise<void> {
    const promptHash = await this.hashPrompt(prompt)
    const promptEmbedding = await this.generateEmbedding(prompt)
    const cost = this.estimateResponseCost(response)
    
    await prisma.$executeRaw`
      INSERT INTO ai_response_cache (
        prompt_hash,
        prompt_embedding,
        response_content,
        confidence_score,
        cost_saved,
        sources_used,
        tier_used,
        hit_count
      ) VALUES (
        ${promptHash},
        ${JSON.stringify(promptEmbedding)}::vector,
        ${response},
        ${confidence},
        ${cost},
        ${JSON.stringify(sources)},
        1,
        1
      )
      ON CONFLICT (prompt_hash) 
      DO UPDATE SET
        hit_count = ai_response_cache.hit_count + 1,
        last_used = now()
    `
  }

  private async updateKnowledgeBase(
    prompt: string,
    response: string,
    confidence: number
  ): Promise<void> {
    // ê³ ì‹ ë¢°ë„ ì‘ë‹µë§Œ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€
    if (confidence < 0.85) return
    
    const embedding = await this.generateEmbedding(prompt + ' ' + response)
    const category = this.categorizeContent(prompt)
    
    await prisma.$executeRaw`
      INSERT INTO knowledge_base (
        content,
        embedding,
        metadata,
        category,
        confidence_score,
        created_at
      ) VALUES (
        ${response},
        ${JSON.stringify(embedding)}::vector,
        ${JSON.stringify({
          originalQuery: prompt,
          category,
          confidence,
          auto_generated: true
        })},
        ${category},
        ${confidence},
        now()
      )
    `
    
    console.log(`ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸: ${category} (ì‹ ë¢°ë„: ${confidence})`)
  }

  private categorizeContent(content: string): string {
    const categories = {
      'formula': ['ìˆ˜ì‹', 'formula', 'vlookup', 'index', 'match', 'if'],
      'data': ['ë°ì´í„°', 'ì •ë ¬', 'í•„í„°', 'í”¼ë²—', 'pivot'],
      'formatting': ['ì„œì‹', 'ì¡°ê±´ë¶€', 'í¬ë§·', 'ìƒ‰ìƒ', 'í°íŠ¸'],
      'vba': ['ë§¤í¬ë¡œ', 'vba', 'ìë™í™”', 'macro'],
      'chart': ['ì°¨íŠ¸', 'ê·¸ë˜í”„', 'chart', 'graph'],
      'error': ['ì˜¤ë¥˜', 'error', 'ì—ëŸ¬', '#ref', '#value', '#name']
    }
    
    const lowerContent = content.toLowerCase()
    
    for (const [category, keywords] of Object.entries(categories)) {
      if (keywords.some(keyword => lowerContent.includes(keyword))) {
        return category
      }
    }
    
    return 'general'
  }

  private async generateEmbedding(text: string): Promise<number[]> {
    // ìºì‹œ í™•ì¸
    const cacheKey = this.hashText(text)
    if (this.embeddingCache.has(cacheKey)) {
      return this.embeddingCache.get(cacheKey)!
    }
    
    // OpenAI ì„ë² ë”© API í˜¸ì¶œ
    try {
      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
        },
        body: JSON.stringify({
          model: 'text-embedding-3-small',
          input: text.slice(0, 8000) // í† í° ì œí•œ
        })
      })
      
      const data = await response.json()
      const embedding = data.data[0].embedding
      
      // ìºì‹œ ì €ì¥
      this.embeddingCache.set(cacheKey, embedding)
      
      return embedding
    } catch (error) {
      console.error('ì„ë² ë”© ìƒì„± ì‹¤íŒ¨:', error)
      // í´ë°±: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
      return this.generateFallbackEmbedding(text)
    }
  }

  private generateFallbackEmbedding(text: string): number[] {
    // ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ í•´ì‹± ê¸°ë°˜ ì„ë² ë”© (1536ì°¨ì›)
    const words = text.toLowerCase().split(/\s+/)
    const embedding = new Array(1536).fill(0)
    
    words.forEach((word, index) => {
      const hash = this.simpleHash(word)
      const pos = hash % 1536
      embedding[pos] += 1 / (index + 1)
    })
    
    // ì •ê·œí™”
    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0))
    return embedding.map(val => val / (magnitude || 1))
  }

  private simpleHash(str: string): number {
    let hash = 0
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash // 32ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜
    }
    return Math.abs(hash)
  }

  private hashText(text: string): string {
    return this.simpleHash(text).toString(36)
  }

  private async hashPrompt(prompt: string): Promise<string> {
    const encoder = new TextEncoder()
    const data = encoder.encode(prompt)
    const hashBuffer = await crypto.subtle.digest('SHA-256', data)
    const hashArray = Array.from(new Uint8Array(hashBuffer))
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('')
  }

  private extractConfidence(response: string): number {
    const match = response.match(/ì‹ ë¢°ë„:\s*(\d*\.?\d+)/i)
    return match ? parseFloat(match[1]) : 0.7
  }

  private estimateCostSaved(prompt: string, sourcesCount: number = 0): number {
    // RAGë¥¼ í†µí•œ ë¹„ìš© ì ˆì•½ ê³„ì‚°
    const baseTokens = prompt.length / 4
    const contextTokens = sourcesCount * 100
    const totalTokens = baseTokens + contextTokens
    
    // Tier 1 ì‚¬ìš©ìœ¼ë¡œ ê°€ì •í•œ ì ˆì•½ ë¹„ìš©
    const tier3Cost = totalTokens * 0.0004 / 1000
    const tier1Cost = totalTokens * 0.00015 / 1000
    
    return tier3Cost - tier1Cost
  }

  private estimateResponseCost(response: string): number {
    const tokens = response.length / 4
    return tokens * 0.00015 / 1000 // Tier 1 ë¹„ìš©
  }

  // ì§€ì‹ ë² ì´ìŠ¤ í†µê³„
  async getKnowledgeStats(): Promise<{
    totalEntries: number
    categoryCounts: Record<string, number>
    averageConfidence: number
    cacheHitRate: number
  }> {
    const stats = await prisma.$queryRaw`
      SELECT 
        COUNT(*) as total_entries,
        AVG(confidence_score) as avg_confidence,
        (SELECT COUNT(*) FROM ai_response_cache WHERE hit_count > 1) as cache_hits,
        (SELECT COUNT(*) FROM ai_response_cache) as total_cache_entries
      FROM knowledge_base
    ` as any[]
    
    const categoryStats = await prisma.$queryRaw`
      SELECT category, COUNT(*) as count
      FROM knowledge_base
      GROUP BY category
    ` as any[]
    
    const categoryCounts = categoryStats.reduce((acc, row) => {
      acc[row.category] = parseInt(row.count)
      return acc
    }, {})
    
    const stat = stats[0]
    const cacheHitRate = stat.total_cache_entries > 0 
      ? (stat.cache_hits / stat.total_cache_entries) * 100 
      : 0
    
    return {
      totalEntries: parseInt(stat.total_entries),
      categoryCounts,
      averageConfidence: parseFloat(stat.avg_confidence || 0),
      cacheHitRate
    }
  }
}

// ê¸€ë¡œë²Œ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
export const ragSystem = new RAGSystem(process.env.OPENROUTER_API_KEY!)

// ì‚¬ìš© ì˜ˆì‹œ
export async function enhanceAnalysisWithRAG(
  prompt: string,
  options: {
    useCache?: boolean
    maxSources?: number
  } = {}
): Promise<RAGEnhancedResponse> {
  return await ragSystem.enhanceWithRAG(prompt, options)
}